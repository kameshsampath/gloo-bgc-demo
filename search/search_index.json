{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"As part of this short tutorial/demo we will be deploying a simple colored hello world microservice and explore how to integrate the service running on an on-premise VM with public cloud clusters using Istio. We will also leverage Gloo Mesh federation capabilites to unify the Istio clusters to provide better interoprablity between the clusters and VM. What we wil be doing as part of this demo ? \u00b6 Setup Kubernetes clusters on three public clouds Setup Virutal Machine on on-premise network Site-to-Site VPN to connect on-premise to public cloud Deploy Gloo Mesh Enterprise on clusters Deploy Istio on clusters Deploy Istio sidecar on VM Traffic Distribution between VM on-premise and public cloud Access Policies to control traffic","title":"Overview"},{"location":"#what-we-wil-be-doing-as-part-of-this-demo","text":"Setup Kubernetes clusters on three public clouds Setup Virutal Machine on on-premise network Site-to-Site VPN to connect on-premise to public cloud Deploy Gloo Mesh Enterprise on clusters Deploy Istio on clusters Deploy Istio sidecar on VM Traffic Distribution between VM on-premise and public cloud Access Policies to control traffic","title":"What we wil be doing as part of this demo ?"},{"location":"access-policies/","text":"In this chapter we will create and apply access policies that will restrict access to services in Gloo Mesh Clusters and VM. Ensure Environment \u00b6 Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc Enable Global Access Policy \u00b6 The Virtual Mesh that we crated in earlier as the globalAccessPolicy disabled, run the following command to verify the state of the same, kubectl --context = \" $MGMT \" get virtualmeshes.networking.mesh.gloo.solo.io -n gloo-mesh bgc-virtual-mesh -o json | jq '.spec.globalAccessPolicy' Now lets enable the globalAccessPolicy by running the following command, $TUTORIAL_HOME /bin/17_toggle_mesh_access_policy.sh As soon as the globalAccessPolicy is enabled you should see no traffic. Now running the following command again should return a response ENABLED , kubectl --context = \" $MGMT \" get virtualmeshes.networking.mesh.gloo.solo.io -n gloo-mesh bgc-virtual-mesh -o json | jq '.spec.globalAccessPolicy' Lets verify if we are able to access the service, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" The command should show an output like: RBAC: access denied Note Refreshing the browser should also return RBAC: acccess denied as shown By default the moment we enable globalAccessPolicy the Gloo Mesh create allowNone with out any access policy associated with the resource, hence you the RBAC: access denied as output when calling the service. Enable Access \u00b6 Ingress Traffic via Istio \u00b6 Let us now allow the traffic via Istio Ingress Gateway, $TUTORIAL_HOME /bin/18_enable_access_via_ingress.sh Let us try polling the service, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should show the output from the service: ################################################### Polling Service URL 35 .244.63.221/api ################################################### { \"color\" : \"blue\" , \"count\" : 476 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 477 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } RBAC: access denied { \"color\" : \"green\" , \"count\" : 2173 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"green\" , \"count\" : 2174 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } As you noticed the command shows one RBAC: access denied , that is from the VM Canary service. Note If you are using the browser then you might see as shown, which is related to no Access to VM. Checking the Istio logs on the VM will show rbac_access_denied_matched_policy , vagrant ssh -c \"sudo tail -f /var/log/istio/istio.log\" [2021-10-02T05:39:53.720Z] \"GET /api HTTP/1.1\" 403 - rbac_access_denied_matched_policy[none] - \"-\" 0 19 0 - \"172.17.0.1\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15\" \"91dfed15-3288-469b-ad93-38a5a6af10f3\" \"34.93.158.36\" \"-\" inbound|8080|| - 192.168.68.119:8080 172.17.0.1:0 outbound_.8080_.version-canary_.blue-green-canary.vm-blue-green-canary.svc.cluster.local - Allow Traffic to VM \u00b6 What we did right now is allowing traffic from the Gloo Mesh Clusters and the traffic from and to vm is still blocked. Now let us now apply the Access policy that will allow the traffic to the VM, $TUTORIAL_HOME /bin/19_enable_access_to_vm.sh Let us try polling the service agian or you could refresh the browser, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should now include the output from the canary(VM) version of service as well, { \"color\" : \"green\" , \"count\" : 2175 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 478 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } ... { \"color\" : \"yellow\" , \"count\" : 197 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.114\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" } Access service from VM \u00b6 Let us now check if we are able to access the service from the VM, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" As expected we got RBAC: access denied as we are yet to allow the traffic from the VM to the cluster, HTTP/1.1 403 Forbidden content-length: 19 content-type: text/plain date: Thu, 23 Sep 2021 16:29:22 GMT server: envoy x-envoy-upstream-service-time: 269 RBAC: access denied Let us allow the VM to access the services on the Cluster, $TUTORIAL_HOME /bin/20_enable_access_from_vm.sh Now trying again to access the service from VM should show a successful response, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" HTTP/1.1 200 OK content-length: 129 content-type: application/json; charset=utf-8 date: Thu, 23 Sep 2021 16:32:22 GMT etag: \"81-44uxFMXdXGKizzXNw4AXYnD7xj0\" server: envoy vary: Accept-Encoding x-envoy-upstream-service-time: 285 { \"color\": \"yellow\", \"count\": 198, \"greeting\": \"Hola \u270b\ud83c\udffd\", \"pod\": \"vm-192.168.68.114\", \"textColor\": \"black\", \"userAgent\": \"HTTPie/2.5.0\" } Voli\u00e0! We have successfully enabled the Access Policies on our Gloo Mesh cluster enabling traffic via Ingress to services on the Cluster and VM.","title":"Access Policies"},{"location":"access-policies/#ensure-environment","text":"Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc","title":"Ensure Environment"},{"location":"access-policies/#enable-global-access-policy","text":"The Virtual Mesh that we crated in earlier as the globalAccessPolicy disabled, run the following command to verify the state of the same, kubectl --context = \" $MGMT \" get virtualmeshes.networking.mesh.gloo.solo.io -n gloo-mesh bgc-virtual-mesh -o json | jq '.spec.globalAccessPolicy' Now lets enable the globalAccessPolicy by running the following command, $TUTORIAL_HOME /bin/17_toggle_mesh_access_policy.sh As soon as the globalAccessPolicy is enabled you should see no traffic. Now running the following command again should return a response ENABLED , kubectl --context = \" $MGMT \" get virtualmeshes.networking.mesh.gloo.solo.io -n gloo-mesh bgc-virtual-mesh -o json | jq '.spec.globalAccessPolicy' Lets verify if we are able to access the service, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" The command should show an output like: RBAC: access denied Note Refreshing the browser should also return RBAC: acccess denied as shown By default the moment we enable globalAccessPolicy the Gloo Mesh create allowNone with out any access policy associated with the resource, hence you the RBAC: access denied as output when calling the service.","title":"Enable Global Access Policy"},{"location":"access-policies/#enable-access","text":"","title":"Enable Access"},{"location":"access-policies/#ingress-traffic-via-istio","text":"Let us now allow the traffic via Istio Ingress Gateway, $TUTORIAL_HOME /bin/18_enable_access_via_ingress.sh Let us try polling the service, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should show the output from the service: ################################################### Polling Service URL 35 .244.63.221/api ################################################### { \"color\" : \"blue\" , \"count\" : 476 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 477 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } RBAC: access denied { \"color\" : \"green\" , \"count\" : 2173 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"green\" , \"count\" : 2174 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } As you noticed the command shows one RBAC: access denied , that is from the VM Canary service. Note If you are using the browser then you might see as shown, which is related to no Access to VM. Checking the Istio logs on the VM will show rbac_access_denied_matched_policy , vagrant ssh -c \"sudo tail -f /var/log/istio/istio.log\" [2021-10-02T05:39:53.720Z] \"GET /api HTTP/1.1\" 403 - rbac_access_denied_matched_policy[none] - \"-\" 0 19 0 - \"172.17.0.1\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15\" \"91dfed15-3288-469b-ad93-38a5a6af10f3\" \"34.93.158.36\" \"-\" inbound|8080|| - 192.168.68.119:8080 172.17.0.1:0 outbound_.8080_.version-canary_.blue-green-canary.vm-blue-green-canary.svc.cluster.local -","title":"Ingress Traffic via Istio"},{"location":"access-policies/#allow-traffic-to-vm","text":"What we did right now is allowing traffic from the Gloo Mesh Clusters and the traffic from and to vm is still blocked. Now let us now apply the Access policy that will allow the traffic to the VM, $TUTORIAL_HOME /bin/19_enable_access_to_vm.sh Let us try polling the service agian or you could refresh the browser, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should now include the output from the canary(VM) version of service as well, { \"color\" : \"green\" , \"count\" : 2175 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 478 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } ... { \"color\" : \"yellow\" , \"count\" : 197 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.114\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" }","title":"Allow Traffic to VM"},{"location":"access-policies/#access-service-from-vm","text":"Let us now check if we are able to access the service from the VM, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" As expected we got RBAC: access denied as we are yet to allow the traffic from the VM to the cluster, HTTP/1.1 403 Forbidden content-length: 19 content-type: text/plain date: Thu, 23 Sep 2021 16:29:22 GMT server: envoy x-envoy-upstream-service-time: 269 RBAC: access denied Let us allow the VM to access the services on the Cluster, $TUTORIAL_HOME /bin/20_enable_access_from_vm.sh Now trying again to access the service from VM should show a successful response, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" HTTP/1.1 200 OK content-length: 129 content-type: application/json; charset=utf-8 date: Thu, 23 Sep 2021 16:32:22 GMT etag: \"81-44uxFMXdXGKizzXNw4AXYnD7xj0\" server: envoy vary: Accept-Encoding x-envoy-upstream-service-time: 285 { \"color\": \"yellow\", \"count\": 198, \"greeting\": \"Hola \u270b\ud83c\udffd\", \"pod\": \"vm-192.168.68.114\", \"textColor\": \"black\", \"userAgent\": \"HTTPie/2.5.0\" } Voli\u00e0! We have successfully enabled the Access Policies on our Gloo Mesh cluster enabling traffic via Ingress to services on the Cluster and VM.","title":"Access service from VM"},{"location":"ansible-roles/","text":"Kamesh Sampath date: 2021-09-30","title":"Ansible Roles"},{"location":"clean-up/","text":"At the end of this chapter you would have, Delete the Cloud resources on Civo, AWS, GCP Delete the Vagrant VM Ensure Environment \u00b6 Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc Cloud Clean up \u00b6 make clean-up Destroy Vagrant VM \u00b6 make vm-destroy","title":"Clean Up"},{"location":"clean-up/#ensure-environment","text":"Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc","title":"Ensure Environment"},{"location":"clean-up/#cloud-clean-up","text":"make clean-up","title":"Cloud Clean up"},{"location":"clean-up/#destroy-vagrant-vm","text":"make vm-destroy","title":"Destroy Vagrant VM"},{"location":"deploy-blue/","text":"Ensure environment \u00b6 Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc Deploy Blue Application \u00b6 $TUTORIAL_HOME /bin/2_deploy_blue.sh \" ${ CLUSTER1 } \" Wait for the blue-green-canary deployment to be running, kubectl --context = \" ${ CLUSTER1 } \" -n blue-green-canary rollout status deploy/blue Let us ensure we got the services and deployments ready, kubectl --context = \" ${ CLUSTER1 } \" -n blue-green-canary get pods,svc NAME READY STATUS RESTARTS AGE pod/green-5c798c7c9d-n4knk 2/2 Running 0 16s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/blue-green-canary ClusterIP 10.100.145.63 <none> 8080/TCP 16s Create Gateway and Virtual Service \u00b6 We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER1 } \" Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Check the Service \u00b6 Call the service to check if its working, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" The command should return an output like, { \"color\" : \"blue\" , \"count\" : 0 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-6bd6f6bf89-2c6c9\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" }","title":"Deploy Blue"},{"location":"deploy-blue/#ensure-environment","text":"Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc","title":"Ensure environment"},{"location":"deploy-blue/#deploy-blue-application","text":"$TUTORIAL_HOME /bin/2_deploy_blue.sh \" ${ CLUSTER1 } \" Wait for the blue-green-canary deployment to be running, kubectl --context = \" ${ CLUSTER1 } \" -n blue-green-canary rollout status deploy/blue Let us ensure we got the services and deployments ready, kubectl --context = \" ${ CLUSTER1 } \" -n blue-green-canary get pods,svc NAME READY STATUS RESTARTS AGE pod/green-5c798c7c9d-n4knk 2/2 Running 0 16s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/blue-green-canary ClusterIP 10.100.145.63 <none> 8080/TCP 16s","title":"Deploy Blue Application"},{"location":"deploy-blue/#create-gateway-and-virtual-service","text":"We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER1 } \" Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' )","title":"Create Gateway and Virtual Service"},{"location":"deploy-blue/#check-the-service","text":"Call the service to check if its working, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" The command should return an output like, { \"color\" : \"blue\" , \"count\" : 0 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-6bd6f6bf89-2c6c9\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" }","title":"Check the Service"},{"location":"deploy-canary/","text":"At the end of this chapter you would have, Deployed Canary App on VM Create VM resources Configure Isito Sidecar Istio Control Plane resources Ensure environment \u00b6 Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc Prepare Istio to Integrate VM Resources \u00b6 For this demo we will make Cluster-1 host the VM resources. For the VM integration we need to have istiod exposed via gateway. We have enabled the istiod service and gateway when we deployed Istio earlier. Let use verify the same by runnig the command, kubectl --context = \" ${ CLUSTER1 } \" get vs,gw -n istio-system NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 20h NAME AGE gateway.networking.istio.io/cross-network-gateway 20h gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 22m gateway.networking.istio.io/istiod-gateway 20h Onboard VM Workloads \u00b6 In the following section we will canary version of the application as systemd service in the VM, deploy Istio Sidecar and integrate that with Istio on the $CLUSTER1 . As we did earlier let us run the following command that will run the requried Ansible tasks to create the blue-green-canary service, install enable and start Istio sidecar on the VM. make workload-run The successful run of workload setup would have created the following resources, VM blue-green-canay service\u2019s Destination and Workload on $MGMT cluster, kubectl --context = $MGMT get destination,workload -n gloo-mesh NAME AGE destination.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster2 7h15m destination.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster1 7h46m destination.discovery.mesh.gloo.solo.io/blue-green-canary-blue-green-canary-cluster1 108m destination.discovery.mesh.gloo.solo.io/blue-green-canary-blue-green-canary-cluster2 97m destination.discovery.mesh.gloo.solo.io/ blue-green-canary-vm-blue-green-canary-cluster1 51m NAME AGE workload.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster1-deployment 7h46m workload.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster2-deployment 7h15m workload.discovery.mesh.gloo.solo.io/blue-blue-green-canary-cluster1-deployment 108m workload.discovery.mesh.gloo.solo.io/green-blue-green-canary-cluster2-deployment 97m workload.discovery.mesh.gloo.solo.io/blue-green-canary-vm-blue-green-canary-cluster1 20s VM blue-green-canay service\u2019s Service and WorkloadEntry on $CLUSTER1 cluster. Note the WorkloadEntry IP address maps to the on-premise VM. kubectl --context = $CLUSTER1 get service,workloadentry -n vm-blue-green-canary -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/blue-green-canary ClusterIP 172.18.8.152 <none> 8080/TCP 74m app=blue-green-canary NAME AGE ADDRESS workloadentry.networking.istio.io/blue-green-canary 74m 192.168.68.119 Important Its is important for the workload entry ADDRESS to map to your local VM address for the integration to work correctly. The Istio service logs /var/log/istio/istio.log should reflect the successful connection with the $CLUSTER1 istiod service (trimmed for brevity), 2021-10-01T16:24:29.624607Z warn citadelclient cannot load key pair, using token instead: open /etc/certs/ cert-chain.pem: no such file or directory 2021-10-01T16:24:29.772439Z info xdsproxy connected to upstream XDS server: istiod.istio-system.svc:1 5012 2021-10-01T16:24:29.854536Z info cache generated new workload certificate latency=312.0443ms ttl=23h5 9m59.145478668s 2021-10-01T16:24:29.854584Z info cache Root cert has changed, start rotating root cert 2021-10-01T16:24:29.854604Z info ads XDS: Incremental Pushing:0 ConnectedEndpoints:0 Version: 2021-10-01T16:24:29.855134Z info cache returned workload trust anchor from cache ttl=23h59m59.144871 73s 2021-10-01T16:24:29.961145Z info ads ADS: new connection for node:sidecar~192.168.68.119~ubuntu-focal.vm -blue-green-canary~vm-blue-green-canary.svc.cluster.local-1 2021-10-01T16:24:29.961220Z info cache returned workload certificate from cache ttl=23h59m59.038785 303s 2021-10-01T16:24:29.961763Z info sds SDS: PUSH resource=default 2021-10-01T16:24:29.962475Z info ads ADS: new connection for node:sidecar~192.168.68.119~ubuntu-focal.vm -blue-green-canary~vm-blue-green-canary.svc.cluster.local-2 2021-10-01T16:24:29.962539Z info cache returned workload trust anchor from cache ttl=23h59m59.037464 545s 2021-10-01T16:24:29.962764Z info sds SDS: PUSH resource=ROOTCA Checking Connectivity \u00b6 With us now having done all the necessary setup that has integrated our VM sidecar with $CLUSTER1 Istio, let us check the connectivity from either side. From VM \u00b6 With globalAccessPolicy disabled in the virtual mesh, calling a Istio service from VM should go through successfully, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" HTTP/ 1.1 200 OK co ntent - le n g t h : 137 co ntent - t ype : applica t io n /jso n ; charse t =u tf -8 da te : Thu , 23 Sep 2021 14 : 06 : 46 GMT e ta g : \"89-coTFNpCn3MGCfccrhUII0cS8+hw\" server : e n voy vary : Accep t -E n codi n g x -e n voy - ups trea m - service - t ime : 179 { \"color\" : \"blue\" , \"count\" : 0 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } To VM \u00b6 We should also be able to access the VM from the cluster as shown, NETWORK_UTILS_POD = $( kubectl --context = \" $CLUSTER1 \" get pods -lapp = network-utils --no-headers | awk '{print $1}' ) printf \"\\nUsing pod %s \\n\" $NETWORK_UTILS_POD kubectl --context = \" $CLUSTER1 \" exec -c network-utils -it $NETWORK_UTILS_POD -- http blue-green-canary.vm-blue-green-canary.svc.cluster.local:8080/api The command should show an output like, HTTP/ 1.1 200 OK co ntent - le n g t h : 127 co ntent - t ype : applica t io n /jso n ; charse t =u tf -8 da te : Sa t , 02 Oc t 2021 04 : 17 : 52 GMT e ta g : \"7f-DeWHCZhRbh4Y6bdigpjrPtHKI3U\" server : e n voy vary : Accep t -E n codi n g x -e n voy - ups trea m - service - t ime : 208 { \"color\" : \"yellow\" , \"count\" : 1 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.119\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" } Awesome! We got the mechnanics working between VM and our Istio on $CLUSTER1 . In upcoming chapters let us apply Gloo Mesh TrafficPolicy to distribute traffic between the blue-green-canary versions on $CLUSTER1 and $CLUSTER2 and the VM.","title":"Deploy Canary"},{"location":"deploy-canary/#ensure-environment","text":"Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc","title":"Ensure environment"},{"location":"deploy-canary/#prepare-istio-to-integrate-vm-resources","text":"For this demo we will make Cluster-1 host the VM resources. For the VM integration we need to have istiod exposed via gateway. We have enabled the istiod service and gateway when we deployed Istio earlier. Let use verify the same by runnig the command, kubectl --context = \" ${ CLUSTER1 } \" get vs,gw -n istio-system NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 20h NAME AGE gateway.networking.istio.io/cross-network-gateway 20h gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 22m gateway.networking.istio.io/istiod-gateway 20h","title":"Prepare Istio to Integrate VM Resources"},{"location":"deploy-canary/#onboard-vm-workloads","text":"In the following section we will canary version of the application as systemd service in the VM, deploy Istio Sidecar and integrate that with Istio on the $CLUSTER1 . As we did earlier let us run the following command that will run the requried Ansible tasks to create the blue-green-canary service, install enable and start Istio sidecar on the VM. make workload-run The successful run of workload setup would have created the following resources, VM blue-green-canay service\u2019s Destination and Workload on $MGMT cluster, kubectl --context = $MGMT get destination,workload -n gloo-mesh NAME AGE destination.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster2 7h15m destination.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster1 7h46m destination.discovery.mesh.gloo.solo.io/blue-green-canary-blue-green-canary-cluster1 108m destination.discovery.mesh.gloo.solo.io/blue-green-canary-blue-green-canary-cluster2 97m destination.discovery.mesh.gloo.solo.io/ blue-green-canary-vm-blue-green-canary-cluster1 51m NAME AGE workload.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster1-deployment 7h46m workload.discovery.mesh.gloo.solo.io/istio-ingressgateway-istio-system-cluster2-deployment 7h15m workload.discovery.mesh.gloo.solo.io/blue-blue-green-canary-cluster1-deployment 108m workload.discovery.mesh.gloo.solo.io/green-blue-green-canary-cluster2-deployment 97m workload.discovery.mesh.gloo.solo.io/blue-green-canary-vm-blue-green-canary-cluster1 20s VM blue-green-canay service\u2019s Service and WorkloadEntry on $CLUSTER1 cluster. Note the WorkloadEntry IP address maps to the on-premise VM. kubectl --context = $CLUSTER1 get service,workloadentry -n vm-blue-green-canary -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/blue-green-canary ClusterIP 172.18.8.152 <none> 8080/TCP 74m app=blue-green-canary NAME AGE ADDRESS workloadentry.networking.istio.io/blue-green-canary 74m 192.168.68.119 Important Its is important for the workload entry ADDRESS to map to your local VM address for the integration to work correctly. The Istio service logs /var/log/istio/istio.log should reflect the successful connection with the $CLUSTER1 istiod service (trimmed for brevity), 2021-10-01T16:24:29.624607Z warn citadelclient cannot load key pair, using token instead: open /etc/certs/ cert-chain.pem: no such file or directory 2021-10-01T16:24:29.772439Z info xdsproxy connected to upstream XDS server: istiod.istio-system.svc:1 5012 2021-10-01T16:24:29.854536Z info cache generated new workload certificate latency=312.0443ms ttl=23h5 9m59.145478668s 2021-10-01T16:24:29.854584Z info cache Root cert has changed, start rotating root cert 2021-10-01T16:24:29.854604Z info ads XDS: Incremental Pushing:0 ConnectedEndpoints:0 Version: 2021-10-01T16:24:29.855134Z info cache returned workload trust anchor from cache ttl=23h59m59.144871 73s 2021-10-01T16:24:29.961145Z info ads ADS: new connection for node:sidecar~192.168.68.119~ubuntu-focal.vm -blue-green-canary~vm-blue-green-canary.svc.cluster.local-1 2021-10-01T16:24:29.961220Z info cache returned workload certificate from cache ttl=23h59m59.038785 303s 2021-10-01T16:24:29.961763Z info sds SDS: PUSH resource=default 2021-10-01T16:24:29.962475Z info ads ADS: new connection for node:sidecar~192.168.68.119~ubuntu-focal.vm -blue-green-canary~vm-blue-green-canary.svc.cluster.local-2 2021-10-01T16:24:29.962539Z info cache returned workload trust anchor from cache ttl=23h59m59.037464 545s 2021-10-01T16:24:29.962764Z info sds SDS: PUSH resource=ROOTCA","title":"Onboard VM Workloads"},{"location":"deploy-canary/#checking-connectivity","text":"With us now having done all the necessary setup that has integrated our VM sidecar with $CLUSTER1 Istio, let us check the connectivity from either side.","title":"Checking Connectivity"},{"location":"deploy-canary/#from-vm","text":"With globalAccessPolicy disabled in the virtual mesh, calling a Istio service from VM should go through successfully, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" HTTP/ 1.1 200 OK co ntent - le n g t h : 137 co ntent - t ype : applica t io n /jso n ; charse t =u tf -8 da te : Thu , 23 Sep 2021 14 : 06 : 46 GMT e ta g : \"89-coTFNpCn3MGCfccrhUII0cS8+hw\" server : e n voy vary : Accep t -E n codi n g x -e n voy - ups trea m - service - t ime : 179 { \"color\" : \"blue\" , \"count\" : 0 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" }","title":"From VM"},{"location":"deploy-canary/#to-vm","text":"We should also be able to access the VM from the cluster as shown, NETWORK_UTILS_POD = $( kubectl --context = \" $CLUSTER1 \" get pods -lapp = network-utils --no-headers | awk '{print $1}' ) printf \"\\nUsing pod %s \\n\" $NETWORK_UTILS_POD kubectl --context = \" $CLUSTER1 \" exec -c network-utils -it $NETWORK_UTILS_POD -- http blue-green-canary.vm-blue-green-canary.svc.cluster.local:8080/api The command should show an output like, HTTP/ 1.1 200 OK co ntent - le n g t h : 127 co ntent - t ype : applica t io n /jso n ; charse t =u tf -8 da te : Sa t , 02 Oc t 2021 04 : 17 : 52 GMT e ta g : \"7f-DeWHCZhRbh4Y6bdigpjrPtHKI3U\" server : e n voy vary : Accep t -E n codi n g x -e n voy - ups trea m - service - t ime : 208 { \"color\" : \"yellow\" , \"count\" : 1 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.119\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" } Awesome! We got the mechnanics working between VM and our Istio on $CLUSTER1 . In upcoming chapters let us apply Gloo Mesh TrafficPolicy to distribute traffic between the blue-green-canary versions on $CLUSTER1 and $CLUSTER2 and the VM.","title":"To VM"},{"location":"deploy-green/","text":"Ensure environment \u00b6 Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc Deploy Green Application \u00b6 $TUTORIAL_HOME /bin/3_deploy_green.sh \" ${ CLUSTER2 } \" Wait for the blue-green-canary deployment to be running, kubectl --context = \" ${ CLUSTER2 } \" -n blue-green-canary rollout status deploy/green Let us ensure we got the services and deployments ready, kubectl --context = \" ${ CLUSTER2 } \" -n blue-green-canary get pods,svc NAME READY STATUS RESTARTS AGE pod/green-6bd6f6bf89-2c6c9 2/2 Running 0 3m5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/blue-green-canary ClusterIP 172.18.10.239 <none> 8080/TCP 3m6s Create Gateway and Virtual Service \u00b6 We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER2 } \" Check the Service \u00b6 Call the service to check if its working, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \" The command should return an output like, { \"color\" : \"green\" , \"count\" : 0 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-5c798c7c9d-n4knk\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" }","title":"Deploy Green"},{"location":"deploy-green/#ensure-environment","text":"Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc","title":"Ensure environment"},{"location":"deploy-green/#deploy-green-application","text":"$TUTORIAL_HOME /bin/3_deploy_green.sh \" ${ CLUSTER2 } \" Wait for the blue-green-canary deployment to be running, kubectl --context = \" ${ CLUSTER2 } \" -n blue-green-canary rollout status deploy/green Let us ensure we got the services and deployments ready, kubectl --context = \" ${ CLUSTER2 } \" -n blue-green-canary get pods,svc NAME READY STATUS RESTARTS AGE pod/green-6bd6f6bf89-2c6c9 2/2 Running 0 3m5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/blue-green-canary ClusterIP 172.18.10.239 <none> 8080/TCP 3m6s","title":"Deploy Green Application"},{"location":"deploy-green/#create-gateway-and-virtual-service","text":"We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER2 } \"","title":"Create Gateway and Virtual Service"},{"location":"deploy-green/#check-the-service","text":"Call the service to check if its working, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \" The command should return an output like, { \"color\" : \"green\" , \"count\" : 0 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-5c798c7c9d-n4knk\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" }","title":"Check the Service"},{"location":"env-setup/","text":"At the end of this chapter you would have, Three Kubernetes Clusters on AWS,GCP and Civo The VM on local environment Installed Gloo Mesh Enterprise Demo Environment \u00b6 The demo requires us to have three Kubernetes clusters and one Virtual Machine. The following tables shows the environment to component matrix. Components GCP AWS CIVO VM Gloo Mesh Cluster Name cluster1 cluster2 mgmt Gloo Mesh Management Gloo Mesh Agent Kubernetes Kubernetes Context Name gke eks civo Istio Blue-Green-Canary Service Navigate to the $TUTORIAL_HOME , cd $TUTORIAL_HOME Ansible Variables File \u00b6 Create the Ansible variables file, make encrypt-vars The command will create a file .local.vars.yml under $TUTORIAL_HOME . Important As we will having sensitive keys in the Ansible variables, the file is by default encrypted and the encryption key is stored in the $TUTORIAL_HOME/.password_file The following table shows the ansible variables used by the demo, please make sure to update the values to suit your settings. The values can be edit using the command, make edit-vars Variable Description Default gcp_vpc_name The name of the VPC that will be created. Same VPC will be used with GKE Cluster kameshs-k8s-vpc gcp_vpn_name The name of the VPN that will be created kameshs-site-to-site-vpn gcp_cred_file The SA JSON file to be copied to VM \u201c$HOME/.config/gcloud/credentials\u201d from the Ansible controller machine gcp_project The GCP project to use gcp_region The GCP region to use asia-south-1 aws_access_key_id The AWS Acess Key aws_secret_access_key The AWS Secret Key aws_region The AWS Region to use ap-south-1 eks_cluster_name The AWS EKS cluster name kameshs-gloo-demos aws_vpc_name The AWS VPC name. The same VPC will be used with EKS cluster kameshs-gloo-demos civo_api_key The CIVO Cloud API Key civo_k3s_cluster_name The CIVO Kubernetes cluster Name gloo-mgmt force_app_install Force install application on VM no clean_istio_vm_files Clean the generated Istio VM files no k8s_context The kubernetes context that will be used to query Istio resources gke gloo_mesh A dictionary of values that is used to set the values required for Gloo Mesh setup The gloo_mesh variable by default has the following values, refer to the component matrix above to understand what this dictionary key/value represents. gloo_mesh : mgmt : cloud : civo k8s_context : civo install_istio : no cluster1 : cloud : gcp k8s_context : gke install_istio : yes cluster2 : cloud : aws k8s_context : eks install_istio : yes VM Setup \u00b6 We will use Vagrant to run and configure our workload VM. The Workload VM also serves as our Ansible target host to run the ansible playbooks. The same VM will be reused for running our blue-green-canary microservice. Important Check the vagrant file public_network and set the names in the list to map to your local interface names. Lets bring up the the VM by running the following command, VAGRANT_BOX = ubuntu/focal64 make vm-up Once the VM is up you should see, .envrc file in the $TUTORIAL_HOME which will have the following variables, export TUTORIAL_HOME = /Users/kameshs/git/kameshsampath/gloo-bgc-demo export ISTIO_VERSION = 1 .10.4 # Kubernetes context name of Gloo Mesh Cluster that will have management components installed export MGMT = civo # Kubernetes context names of Gloo Mesh Cluster that will have Istio workload installed, the cluster names are automatically generated during the make create-kubernetes-clusters task export CLUSTER1 = gke export CLUSTER2 = eks export KUBECONFIG = \" ${ TUTORIAL_HOME } /.kube/config\" # AWS export AWS_ACCESS_KEY_ID = <your AWS Access key> export AWS_SECRET_ACCESS_KEY = <your AWS Secret Access key> export AWS_DEFAULT_REGION = <your AWS Region> # CIVO export CIVO_API_KEY = <your civo API key> # GKE export GOOGLE_APPLICATION_CREDENTIALS = /Users/kameshs/.ssh/kameshs-solo-io-gcp.json export CLOUDSDK_CORE_PROJECT = <your GCP Project> export CLOUDSDK_COMPUTE_REGION = <your GCP compute region> # just a single node cluster export CLOUDSDK_COMPUTE_ZONE = <your GCP compute zone> export GKE_CLUSTER_NAME = <your GKE cluster name> # Istio export ISTIO_HOME = \" $TUTORIAL_HOME /istio-1.10.4\" # VM and Istio export VM_APP = blue-green-canary export VM_NAMESPACE = vm-blue-green-canary # this directory will be inside the vm export WORK_DIR = /home/vagrant/istio-vm/files export SERVICE_ACCOUNT = vm-blue-green-canary export CLUSTER_NETWORK = bgc-network1 export VM_NETWORK = cluster1 export CLUSTER = cluster1 # Vagrant export VAULT_FILE = .password_file export VAGRANT_DEFAULT_PROVIDER = virtualbox # tested with ubuntu/focal64, fedora/34-cloud-base export VAGRANT_BOX = ubuntu/focal64 Tip A vary handy tool to work with environment and variables direnv , if you have direnv installed, it will recogonize and load the variables automatically. .kube folder under $TUTORIAL_HOME this will hold the kube config files of all the kube clusters that will be created. Kubernetes Setup \u00b6 The cloud setup will setup the Clouds and create Kubernetes clusters on them. make create-kube-clusters Note The step above will take approx 30 mins to complete depending on the cloud and region. The successful creation of Kube clusters will have the respective kubeconfig files in $TUTORIAL_HOME/.kube . The script will also create a merged kubeconfig file $TUTORIAL_HOME/.kube/config that will have all three Kubernetes contexts merged into single file. Verify the same by running the following command, kubectl config get-contexts -o name The command should show an output like, civo eks gke Verify if you are able to connect to clusters by running the following commands, e.g. to get nodes of gke cluster, KUBECONFIG = $TUTORIAL_HOME /.kube/config kubectl --context = gke get nodes Tip You can also do source $TUTORIAL_HOME/.envrc to have all the variables setup for your current terminal shell Deploy Istio \u00b6 As our clusters are ready, we can deploy Istio on to our workload clusters. Before we deploy lets check our gloo_mesh dictionary on the mappings, gloo_mesh : mgmt : cloud : civo k8s_context : civo install_istio : no cluster1 : cloud : gcp k8s_context : gke install_istio : yes cluster2 : cloud : aws k8s_context : eks install_istio : yes with our settings by runing the following command we should have Istio setup on $CLUSTER1 and $CLUSTER2 . Important As we have setup the aliases via Environment variables, $CLUSTER1=gke $CLUSTER2=aws $MGMT=civo We will use environment variables to refer to them. Rest of the tutorial we will refer the clusters using their environment aliases. You can always check the $TUTORIAL_HOME/.envrc for these mappings. make deploy-istio Verify istio on $CLUSTER1 (gke): kubectl --context = $CLUSTER1 get pods,svc -n istio-system Install Istio on $CLUSTER2 (eks): kubectl --context = $CLUSTER2 get pods,svc -n istio-system Both the comands should return a similar output like, NAME READY STATUS RESTARTS AGE pod/istio-ingressgateway-7cb8f9c54-r84m6 1 /1 Running 0 32m pod/istiod-85d66d64d6-mdgz2 1 /1 Running 0 32m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/istio-ingressgateway LoadBalancer 172 .18.3.206 34 .93.158.36 80 :32614/TCP,443:32739/TCP,15021:31949/TCP,15443:30942/TCP,15012:31256/TCP,15017:31886/TCP 32m service/istiod ClusterIP 172 .18.4.55 <none> 15010 /TCP,15012/TCP,443/TCP,15014/TCP 32m We also deployed a Gateway to istiod to enable the VM to communicate to the Istio cluster, we can query the resources using the command, kubectl --context = $CLUSTER1 get vs,gw -n istio-system Should show an output like, NAME AGE gateway.networking.istio.io/cross-network-gateway 31m gateway.networking.istio.io/istiod-gateway 31m NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 31m Install Gloo Mesh \u00b6 Ensure Enviroment \u00b6 Make sure you have the Gloo Mesh Gateway Enterprise License Key before proceeding to install. Export the license key to variable, export GLOO_MESH_GATEWAY_LICENSE_KEY = <your Gloo Mesh Gateway EE License Key> Set the required environment variables, source $TUTORIAL_HOME /.envrc Download meshctl \u00b6 Download and install latest meshctl by running, curl -sL https://run.solo.io/meshctl/install | sh Add meshctl to the system path, export PATH = $HOME /.gloo-mesh/bin: $PATH Set cluster environment variables source $TUTORIAL_HOME /.envrc Navigate to management work folder, cd $TUTORIAL_HOME /work/mgmt Install Gloo Mesh Enterprise \u00b6 Create the gloo-mesh namespace to install Gloo Mesh , $TUTORIAL_HOME /bin/install_mesh.sh Note You can safely ignore the helm warnings It will take few minutes for the Gloo Mesh to be installed and ready Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status \ud83d\udfe1 Gloo Mesh Agents Connectivity Hints: * No registered clusters detected. To register a remote cluster that has a deployed Gloo Mesh agent, add a KubernetesCluster CR. For more info, see: https://docs.solo.io/gloo-mesh/latest/setup/cluster_registration/enterprise_cluster_registration/ Management Configuration --------------------------- 2021-09-14T06:41:10.594255Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources Cluster Registrations \u00b6 To connect our hetregenous Isito clusters, we need to register them with the management cluster. The cluster registration will communicate with Management cluster via an enterprise-agent , we need to pass the enterprise-agent address as part of cluster registrations to enable effective communcation. Register $CLUSTER1 as cluster1 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER1 cluster1 Register $CLUSTER2 as cluster2 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER2 cluster2 Verify Install \u00b6 Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status +----------+------------+-------------------------------+-----------------+ | CLUSTER | REGISTERED | DASHBOARDS AND AGENTS PULLING | AGENTS PUSHING | +----------+------------+-------------------------------+-----------------+ | cluster1 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ | cluster2 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ \ud83d\udfe2 Gloo Mesh Agents Connectivity Management Configuration --------------------------- 2021-09-14T04:56:04.288180Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources","title":"Environment Setup"},{"location":"env-setup/#demo-environment","text":"The demo requires us to have three Kubernetes clusters and one Virtual Machine. The following tables shows the environment to component matrix. Components GCP AWS CIVO VM Gloo Mesh Cluster Name cluster1 cluster2 mgmt Gloo Mesh Management Gloo Mesh Agent Kubernetes Kubernetes Context Name gke eks civo Istio Blue-Green-Canary Service Navigate to the $TUTORIAL_HOME , cd $TUTORIAL_HOME","title":"Demo Environment"},{"location":"env-setup/#ansible-variables-file","text":"Create the Ansible variables file, make encrypt-vars The command will create a file .local.vars.yml under $TUTORIAL_HOME . Important As we will having sensitive keys in the Ansible variables, the file is by default encrypted and the encryption key is stored in the $TUTORIAL_HOME/.password_file The following table shows the ansible variables used by the demo, please make sure to update the values to suit your settings. The values can be edit using the command, make edit-vars Variable Description Default gcp_vpc_name The name of the VPC that will be created. Same VPC will be used with GKE Cluster kameshs-k8s-vpc gcp_vpn_name The name of the VPN that will be created kameshs-site-to-site-vpn gcp_cred_file The SA JSON file to be copied to VM \u201c$HOME/.config/gcloud/credentials\u201d from the Ansible controller machine gcp_project The GCP project to use gcp_region The GCP region to use asia-south-1 aws_access_key_id The AWS Acess Key aws_secret_access_key The AWS Secret Key aws_region The AWS Region to use ap-south-1 eks_cluster_name The AWS EKS cluster name kameshs-gloo-demos aws_vpc_name The AWS VPC name. The same VPC will be used with EKS cluster kameshs-gloo-demos civo_api_key The CIVO Cloud API Key civo_k3s_cluster_name The CIVO Kubernetes cluster Name gloo-mgmt force_app_install Force install application on VM no clean_istio_vm_files Clean the generated Istio VM files no k8s_context The kubernetes context that will be used to query Istio resources gke gloo_mesh A dictionary of values that is used to set the values required for Gloo Mesh setup The gloo_mesh variable by default has the following values, refer to the component matrix above to understand what this dictionary key/value represents. gloo_mesh : mgmt : cloud : civo k8s_context : civo install_istio : no cluster1 : cloud : gcp k8s_context : gke install_istio : yes cluster2 : cloud : aws k8s_context : eks install_istio : yes","title":"Ansible Variables File"},{"location":"env-setup/#vm-setup","text":"We will use Vagrant to run and configure our workload VM. The Workload VM also serves as our Ansible target host to run the ansible playbooks. The same VM will be reused for running our blue-green-canary microservice. Important Check the vagrant file public_network and set the names in the list to map to your local interface names. Lets bring up the the VM by running the following command, VAGRANT_BOX = ubuntu/focal64 make vm-up Once the VM is up you should see, .envrc file in the $TUTORIAL_HOME which will have the following variables, export TUTORIAL_HOME = /Users/kameshs/git/kameshsampath/gloo-bgc-demo export ISTIO_VERSION = 1 .10.4 # Kubernetes context name of Gloo Mesh Cluster that will have management components installed export MGMT = civo # Kubernetes context names of Gloo Mesh Cluster that will have Istio workload installed, the cluster names are automatically generated during the make create-kubernetes-clusters task export CLUSTER1 = gke export CLUSTER2 = eks export KUBECONFIG = \" ${ TUTORIAL_HOME } /.kube/config\" # AWS export AWS_ACCESS_KEY_ID = <your AWS Access key> export AWS_SECRET_ACCESS_KEY = <your AWS Secret Access key> export AWS_DEFAULT_REGION = <your AWS Region> # CIVO export CIVO_API_KEY = <your civo API key> # GKE export GOOGLE_APPLICATION_CREDENTIALS = /Users/kameshs/.ssh/kameshs-solo-io-gcp.json export CLOUDSDK_CORE_PROJECT = <your GCP Project> export CLOUDSDK_COMPUTE_REGION = <your GCP compute region> # just a single node cluster export CLOUDSDK_COMPUTE_ZONE = <your GCP compute zone> export GKE_CLUSTER_NAME = <your GKE cluster name> # Istio export ISTIO_HOME = \" $TUTORIAL_HOME /istio-1.10.4\" # VM and Istio export VM_APP = blue-green-canary export VM_NAMESPACE = vm-blue-green-canary # this directory will be inside the vm export WORK_DIR = /home/vagrant/istio-vm/files export SERVICE_ACCOUNT = vm-blue-green-canary export CLUSTER_NETWORK = bgc-network1 export VM_NETWORK = cluster1 export CLUSTER = cluster1 # Vagrant export VAULT_FILE = .password_file export VAGRANT_DEFAULT_PROVIDER = virtualbox # tested with ubuntu/focal64, fedora/34-cloud-base export VAGRANT_BOX = ubuntu/focal64 Tip A vary handy tool to work with environment and variables direnv , if you have direnv installed, it will recogonize and load the variables automatically. .kube folder under $TUTORIAL_HOME this will hold the kube config files of all the kube clusters that will be created.","title":"VM Setup"},{"location":"env-setup/#kubernetes-setup","text":"The cloud setup will setup the Clouds and create Kubernetes clusters on them. make create-kube-clusters Note The step above will take approx 30 mins to complete depending on the cloud and region. The successful creation of Kube clusters will have the respective kubeconfig files in $TUTORIAL_HOME/.kube . The script will also create a merged kubeconfig file $TUTORIAL_HOME/.kube/config that will have all three Kubernetes contexts merged into single file. Verify the same by running the following command, kubectl config get-contexts -o name The command should show an output like, civo eks gke Verify if you are able to connect to clusters by running the following commands, e.g. to get nodes of gke cluster, KUBECONFIG = $TUTORIAL_HOME /.kube/config kubectl --context = gke get nodes Tip You can also do source $TUTORIAL_HOME/.envrc to have all the variables setup for your current terminal shell","title":"Kubernetes Setup"},{"location":"env-setup/#deploy-istio","text":"As our clusters are ready, we can deploy Istio on to our workload clusters. Before we deploy lets check our gloo_mesh dictionary on the mappings, gloo_mesh : mgmt : cloud : civo k8s_context : civo install_istio : no cluster1 : cloud : gcp k8s_context : gke install_istio : yes cluster2 : cloud : aws k8s_context : eks install_istio : yes with our settings by runing the following command we should have Istio setup on $CLUSTER1 and $CLUSTER2 . Important As we have setup the aliases via Environment variables, $CLUSTER1=gke $CLUSTER2=aws $MGMT=civo We will use environment variables to refer to them. Rest of the tutorial we will refer the clusters using their environment aliases. You can always check the $TUTORIAL_HOME/.envrc for these mappings. make deploy-istio Verify istio on $CLUSTER1 (gke): kubectl --context = $CLUSTER1 get pods,svc -n istio-system Install Istio on $CLUSTER2 (eks): kubectl --context = $CLUSTER2 get pods,svc -n istio-system Both the comands should return a similar output like, NAME READY STATUS RESTARTS AGE pod/istio-ingressgateway-7cb8f9c54-r84m6 1 /1 Running 0 32m pod/istiod-85d66d64d6-mdgz2 1 /1 Running 0 32m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/istio-ingressgateway LoadBalancer 172 .18.3.206 34 .93.158.36 80 :32614/TCP,443:32739/TCP,15021:31949/TCP,15443:30942/TCP,15012:31256/TCP,15017:31886/TCP 32m service/istiod ClusterIP 172 .18.4.55 <none> 15010 /TCP,15012/TCP,443/TCP,15014/TCP 32m We also deployed a Gateway to istiod to enable the VM to communicate to the Istio cluster, we can query the resources using the command, kubectl --context = $CLUSTER1 get vs,gw -n istio-system Should show an output like, NAME AGE gateway.networking.istio.io/cross-network-gateway 31m gateway.networking.istio.io/istiod-gateway 31m NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 31m","title":"Deploy Istio"},{"location":"env-setup/#install-gloo-mesh","text":"","title":"Install Gloo Mesh"},{"location":"env-setup/#ensure-enviroment","text":"Make sure you have the Gloo Mesh Gateway Enterprise License Key before proceeding to install. Export the license key to variable, export GLOO_MESH_GATEWAY_LICENSE_KEY = <your Gloo Mesh Gateway EE License Key> Set the required environment variables, source $TUTORIAL_HOME /.envrc","title":"Ensure Enviroment"},{"location":"env-setup/#download-meshctl","text":"Download and install latest meshctl by running, curl -sL https://run.solo.io/meshctl/install | sh Add meshctl to the system path, export PATH = $HOME /.gloo-mesh/bin: $PATH Set cluster environment variables source $TUTORIAL_HOME /.envrc Navigate to management work folder, cd $TUTORIAL_HOME /work/mgmt","title":"Download meshctl"},{"location":"env-setup/#install-gloo-mesh-enterprise","text":"Create the gloo-mesh namespace to install Gloo Mesh , $TUTORIAL_HOME /bin/install_mesh.sh Note You can safely ignore the helm warnings It will take few minutes for the Gloo Mesh to be installed and ready Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status \ud83d\udfe1 Gloo Mesh Agents Connectivity Hints: * No registered clusters detected. To register a remote cluster that has a deployed Gloo Mesh agent, add a KubernetesCluster CR. For more info, see: https://docs.solo.io/gloo-mesh/latest/setup/cluster_registration/enterprise_cluster_registration/ Management Configuration --------------------------- 2021-09-14T06:41:10.594255Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources","title":"Install Gloo Mesh Enterprise"},{"location":"env-setup/#cluster-registrations","text":"To connect our hetregenous Isito clusters, we need to register them with the management cluster. The cluster registration will communicate with Management cluster via an enterprise-agent , we need to pass the enterprise-agent address as part of cluster registrations to enable effective communcation. Register $CLUSTER1 as cluster1 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER1 cluster1 Register $CLUSTER2 as cluster2 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER2 cluster2","title":"Cluster Registrations"},{"location":"env-setup/#verify-install","text":"Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status +----------+------------+-------------------------------+-----------------+ | CLUSTER | REGISTERED | DASHBOARDS AND AGENTS PULLING | AGENTS PUSHING | +----------+------------+-------------------------------+-----------------+ | cluster1 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ | cluster2 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ \ud83d\udfe2 Gloo Mesh Agents Connectivity Management Configuration --------------------------- 2021-09-14T04:56:04.288180Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources","title":"Verify Install"},{"location":"mesh/","text":"At the end of this chapter you would have, Created Virtual Mesh to connect $CLUSTER1(gke) and $CLUSTER1(eks) Applied Access Policies Create TrafficPolicy to distribute traffic Integrated VM Workload with Mesh Pre-requsites and Assumptions \u00b6 This chapter assumes that you have done the following if not please vist the earlier chapters to compelte the requriements, Setup VM Created Kubernetes Clusters on $CLUSTER1(gke) , $CLUSTER1(eks) and $MGMT(civo) Deployed Istio on Workload Clusters Installed Gloo Mesh and Registered the clusters Created VPN Tunnel in GCP Ensure environment \u00b6 Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc Enable PeerAuthentication \u00b6 Let us configure Istio PeerAuthentication in $CLUSTER1 and $CLUSTER2 . PeerAuthentication enable the mTLS between service mesh services and will help in unifying the ROOT CA between heterogenous service meshes. This common ROOT CA enables the services across the meshes to trust each other. Cluster 1 \u00b6 kubectl --context = ${ CLUSTER1 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml Cluster 2 \u00b6 kubectl --context = ${ CLUSTER2 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml Virtual Mesh \u00b6 Having enabled mTLS on $CLUSTER1 and $CLUSTER2 , we can now unify the service meshes using Gloo Mesh\u2019s VirtualMesh , kubectl --context = ${ MGMT } apply -f $TUTORIAL_HOME /mesh-files/bgc-virtual-mesh.yaml Gloo Mesh Dashboard \u00b6 We can use Gloo Mesh Dashboard to verify our registered clusters and mesh details, Open a new terminal and run the following command, source $TUTORIAL_HOME /.envrc kubectl --context = $MGMT port-forward -n gloo-mesh deployment/dashboard 8090 :8090 You can then open the dashboard in your browser which will open a page as shown: If you navigate to the Debug tab you could see the bgc-virtual-mesh that we created with cluster1 and cluster2 Istio servicemeshes, Now we are all set to deploy the demo applications on these clusters.","title":"Setup Gloo Mesh"},{"location":"mesh/#pre-requsites-and-assumptions","text":"This chapter assumes that you have done the following if not please vist the earlier chapters to compelte the requriements, Setup VM Created Kubernetes Clusters on $CLUSTER1(gke) , $CLUSTER1(eks) and $MGMT(civo) Deployed Istio on Workload Clusters Installed Gloo Mesh and Registered the clusters Created VPN Tunnel in GCP","title":"Pre-requsites and Assumptions"},{"location":"mesh/#ensure-environment","text":"Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc","title":"Ensure environment"},{"location":"mesh/#enable-peerauthentication","text":"Let us configure Istio PeerAuthentication in $CLUSTER1 and $CLUSTER2 . PeerAuthentication enable the mTLS between service mesh services and will help in unifying the ROOT CA between heterogenous service meshes. This common ROOT CA enables the services across the meshes to trust each other.","title":"Enable PeerAuthentication"},{"location":"mesh/#cluster-1","text":"kubectl --context = ${ CLUSTER1 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml","title":"Cluster 1"},{"location":"mesh/#cluster-2","text":"kubectl --context = ${ CLUSTER2 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml","title":"Cluster 2"},{"location":"mesh/#virtual-mesh","text":"Having enabled mTLS on $CLUSTER1 and $CLUSTER2 , we can now unify the service meshes using Gloo Mesh\u2019s VirtualMesh , kubectl --context = ${ MGMT } apply -f $TUTORIAL_HOME /mesh-files/bgc-virtual-mesh.yaml","title":"Virtual Mesh"},{"location":"mesh/#gloo-mesh-dashboard","text":"We can use Gloo Mesh Dashboard to verify our registered clusters and mesh details, Open a new terminal and run the following command, source $TUTORIAL_HOME /.envrc kubectl --context = $MGMT port-forward -n gloo-mesh deployment/dashboard 8090 :8090 You can then open the dashboard in your browser which will open a page as shown: If you navigate to the Debug tab you could see the bgc-virtual-mesh that we created with cluster1 and cluster2 Istio servicemeshes, Now we are all set to deploy the demo applications on these clusters.","title":"Gloo Mesh Dashboard"},{"location":"site-to-site-vpn/","text":"One of the main goals of this demo is to integrate VM on-premise with public cloud using Istio. To be able to integrate the VM with Istio on the cloud we need to have networking done between public cloud and on-premise. There are many ways to do setup the networking for the demo sake we will have use site-to-site VPN on GCP. Let\u2019s take stock of our inventory, We have setup Kubernetes Clusters on AWS /Civo/ GKE Installed Istio on to AWS / GKE clusters A linux VM provisioned via Vagrant. At the end of this chapter we would have, Created site-site VPN tunnel between GCP and your home network Configured VM with IpSec service to tunnel traffic Demo Architecture \u00b6 Create VPN Resources \u00b6 Important It is highly recommended that you have static IP that will connect your Home network to GCP. Dynamic IP will work but then it might require to VPN re-run the configuration every time the IP changes. When we set the GKE we created the VPC and used it while setting up the GKE as a VPC-native cluster . IP Plan \u00b6 The following section shows the IP plan of the GKE , VPC Network CIDR - 172.16.0.0/12 Subnet Primary Address Range (Nodes) - 172.16.0.0/28 Address: 172.16.0.0 10101100.00010000.00000000.0000 0000 Netmask: 255.255.255.240 = 28 11111111.11111111.11111111.1111 0000 Wildcard: 0.0.0.15 00000000.00000000.00000000.0000 1111 => Network: 172.16.0.0/28 10101100.00010000.00000000.0000 0000 HostMin: 172.16.0.1 10101100.00010000.00000000.0000 0001 HostMax: 172.16.0.14 10101100.00010000.00000000.0000 1110 Broadcast: 172.16.0.15 10101100.00010000.00000000.0000 1111 Hosts/Net: 14 Class B, Private Internet Subnet Secondary Address Range (Pods) - 172.17.0.0/16 Address: 172.17.0.0 10101100.00010001. 00000000.00000000 Netmask: 255.255.0.0 = 16 11111111.11111111. 00000000.00000000 Wildcard: 0.0.255.255 00000000.00000000. 11111111.11111111 => Network: 172.17.0.0/16 10101100.00010001. 00000000.00000000 HostMin: 172.17.0.1 10101100.00010001. 00000000.00000001 HostMax: 172.17.255.254 10101100.00010001. 11111111.11111110 Broadcast: 172.17.255.255 10101100.00010001. 11111111.11111111 Hosts/Net: 65534 Class B, Private Internet Subnet Secondary Address Range (Services) - 172.18.0.0/20 Address: 172.18.0.0 10101100.00010010.0000 0000.00000000 Netmask: 255.255.240.0 = 20 11111111.11111111.1111 0000.00000000 Wildcard: 0.0.15.255 00000000.00000000.0000 1111.11111111 => Network: 172.18.0.0/20 10101100.00010010.0000 0000.00000000 HostMin: 172.18.0.1 10101100.00010010.0000 0000.00000001 HostMax: 172.18.15.254 10101100.00010010.0000 1111.11111110 Broadcast: 172.18.15.255 10101100.00010010.0000 1111.11111111 Hosts/Net: 4094 Class B, Private Internet Running the following command will create the VPN on GCP and configure the site-to-site VPN using strongswan using IPSec . make create-tunnel The successful Gateway and Tunnel creation should show the following resources in the GCP console, Cloud VPN Tunnel Cloud VPN Tunnel creates a VPN tunnel between your on-premise(home) network and Google Cloud. As you notice the tunnel\u2019s status is First handshake as we are yet to initate the connection from your VM which we will be doing in the next section. VPN Gateway The VPN gateway helps routing the VPN traffic from our on-premise network into Google Cloud, Connect VM to Google Cloud \u00b6 The setup done in the previous step would have enabled the strongswan systemd service on the vm let us check the status of the same, Open a new terminal and ssh into the Vagrant vm cd $TUTORIAL_HOME All the commands in this chapter will be executed inside the VM, so lets SSH into it, vagrant ssh IPSec Service \u00b6 The make create-tunnel command would have enabled and started a systemd service called strongswan . Run the following command to check the status of the strongswan service, sudo systemctl status strongswan \u25cf strongswan.service - strongSwan IPsec IKEv1/IKEv2 daemon using swanctl Loaded: loaded ( /lib/systemd/system/strongswan.service ; enabled ; vendor preset: enabled ) Active: active ( running ) since Fri 2021 -10-01 13 :26:48 UTC ; 2s ago Process: 63691 ExecStartPost = /usr/sbin/swanctl --load-all --noprompt ( code = exited, status = 0 /SUCCESS ) Main PID: 63658 ( charon-systemd ) Status: \"charon-systemd running, strongSwan 5.8.2, Linux 5.4.0-88-generic, x86_64\" Tasks: 17 ( limit: 4682 ) Memory: 3 .1M CGroup: /system.slice/strongswan.service \u2514\u250063658 /usr/sbin/charon-systemd Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : selected peer config 'gw-gw' Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : authentication of 'xx.xxx.xx.xxx' with pre-shared key successful Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : authentication of 'xx.xx.xxx.xx' ( myself ) with pre-shared key Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : IKE_SA gw-gw [ 3 ] established between 192 .168.68.119 [ xx.xx.xxx.xx ] ...xx.xxx.xx.xxx [ xx.xxx.xx.xxx ] Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : scheduling reauthentication in 10593s Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : maximum IKE_SA lifetime 11673s Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : selected proposal: ESP:AES_GCM_16_256/NO_EXT_SEQ Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : CHILD_SA home-gcp { 2 } established with SPIs ceb36c53_i c82d0bb7_o and TS 192 .168.0.0/16 === 172 .16.0.0/28 172 .> Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : generating IKE_AUTH response 1 [ IDr AUTH SA TSi TSr N ( AUTH_LFT ) ] Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : sending packet: from 192 .168.68.119 [ 4500 ] to xx.xxx.xx.xxx [ 4500 ] ( 269 bytes ) The service status shows the connection has been successfully loaded, but not established yet. IPSec Service Configuration \u00b6 The strongswan IPSec tunnel connection configuration done using the file /etc/swanctl/conf.d/gcp-gloo-demos.conf . For more details about the configuration please check the strongswan docs . connections { gw -gw { local_addrs = 192.168.68.119 remote_addrs = xx.xxx.xxx.xxx local { auth = psk } remote { auth = psk } children { home -gcp { local_ts = 192.168.0.0/16 remote_ts = 172.16.0.0/28 ,172.17.0.0/16,172.18.0.0/20 rekey_time = 5400 rekey_bytes = 500000000 rekey_packets = 1000000 esp_proposals = aes256gcm16-sha512-modp8192 start_action = start dpd_action = restart } } version = 2 mobike = no reauth_time = 10800 proposals = aes256gcm16-sha512-modp4096 } } secrets { ike -1 { id -1 = 192.168.68.119 id -2 = xx.xxx.xxx.xxx secret = <Generated IPSec Secret> } } Points to Ponder The remote_ts is mapped to the GKE k8s subnets that we had defined for Nodes, Pods and Service respectively. This will enable the VM to route the traffic to those subnets via the VPN gateway and VPN tunnel Its very important to have the Cipher 1 proposals and esp_proposals inline with whats supported by GCP IKE Ciphers 2 . Check the names connections \u2192 gw-gw , children \u2192 home-gcp and IKEV2 3 secret \u2192 ike-1 from the gcp-gloo-demos.conf configuration to their statuses from the strongswan service logs. For more detailed logs use sudo journalctl -xe -u strongswan Check Status \u00b6 The status of tunnel can be checked on the VM using the swanctl utility, sudo swantctl --list-sas The command above lists the SA as shown, vagrant@ubuntu-focal:~$ sudo swanctl --list-sas gw-gw: #3, ESTABLISHED, IKEv2, 63c3d5bc28f7d6eb_i a80e425a5246e759_r* local 'xx.xxx.xx.xx' @ 192.168.68.119[4500] remote 'xx.xxx.xx.xx' @ xx.xxx.xx.xx[4500] AES_GCM_16-256/PRF_HMAC_SHA2_512/MODP_4096 established 194s ago, reauth in 9809s home-gcp: #2, reqid 1, INSTALLED, TUNNEL-in-UDP, ESP:AES_GCM_16-256 installed 194s ago, rekeying in 4814s, expires in 5746s in c23db81e, 1205 bytes, 6 packets, 119s ago out b4a96c26, 531 bytes, 8 packets, 119s ago local 192.168.0.0/16 remote 172.16.0.0/28 172.17.0.0/16 172.18.0.0/20 When checking the Google Cloud Console it should show the VPN Tunnel status as Established , Check Connectivity \u00b6 Let us check the connections between GKE and VM, kubectl --context = gke apply -f $TUTORIAL_HOME /extras/checks-and-tests.yaml Ping from the pod, kubectl --context = gke exec -it $( kubectl --context = gke get pods -lapp = network-utils -ojsonpath = '{.items[*].metadata.name}' ) -- ping <your VM public ip> The ping should successfull with the output like, # e.g kubectl --context=gke exec -it $(kubectl --context=gke get pods -lapp=network-utils -ojsonpath='{.items[*].metadata.name}') -- ping 192.168.68.119 PING 192 .168.68.119 ( 192 .168.68.119 ) 56 ( 84 ) bytes of data. 64 bytes from 192 .168.68.119: icmp_seq = 1 ttl = 62 time = 32 .6 ms 64 bytes from 192 .168.68.119: icmp_seq = 2 ttl = 62 time = 34 .4 ms 64 bytes from 192 .168.68.119: icmp_seq = 3 ttl = 62 time = 117 ms If tunnel was established successfully the following curl should return an HTML response as shown, curl $( kubectl --context = gke get pods -lapp = nginx -ojsonpath = '{.items[*].status.podIP}' ) <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > < style > html { color-scheme : light dark ; } body { width : 35 em ; margin : 0 auto ; font-family : Tahoma , Verdana , Arial , sans-serif ; } </ style > </ head > < body > < h1 > Welcome to nginx! </ h1 > < p > If you see this page, the nginx web server is successfully installed and working. Further configuration is required. </ p > < p > For online documentation and support please refer to < a href = \"http://nginx.org/\" > nginx.org </ a > . < br /> Commercial support is available at < a href = \"http://nginx.com/\" > nginx.com </ a > . </ p > < p >< em > Thank you for using nginx. </ em ></ p > </ body > </ html > When Tunnel is inactive The VPN tunnel goes inactive if there is no activity for sometime, whenever you see the Tunnel is inactive say you are not able to ping the Pods, try restarting the strongswan service and initiate the connection, vagrant ssh -c \"sudo systemctl restart strongswan\" https://wiki.strongswan.org/projects/strongswan/wiki/IKEv2CipherSuites \u21a9 https://cloud.google.com/network-connectivity/docs/vpn/concepts/supported-ike-ciphers \u21a9 https://en.wikipedia.org/wiki/Internet_Key_Exchange \u21a9","title":"Site to Site VPN"},{"location":"site-to-site-vpn/#demo-architecture","text":"","title":"Demo Architecture"},{"location":"site-to-site-vpn/#create-vpn-resources","text":"Important It is highly recommended that you have static IP that will connect your Home network to GCP. Dynamic IP will work but then it might require to VPN re-run the configuration every time the IP changes. When we set the GKE we created the VPC and used it while setting up the GKE as a VPC-native cluster .","title":"Create VPN Resources"},{"location":"site-to-site-vpn/#ip-plan","text":"The following section shows the IP plan of the GKE , VPC Network CIDR - 172.16.0.0/12 Subnet Primary Address Range (Nodes) - 172.16.0.0/28 Address: 172.16.0.0 10101100.00010000.00000000.0000 0000 Netmask: 255.255.255.240 = 28 11111111.11111111.11111111.1111 0000 Wildcard: 0.0.0.15 00000000.00000000.00000000.0000 1111 => Network: 172.16.0.0/28 10101100.00010000.00000000.0000 0000 HostMin: 172.16.0.1 10101100.00010000.00000000.0000 0001 HostMax: 172.16.0.14 10101100.00010000.00000000.0000 1110 Broadcast: 172.16.0.15 10101100.00010000.00000000.0000 1111 Hosts/Net: 14 Class B, Private Internet Subnet Secondary Address Range (Pods) - 172.17.0.0/16 Address: 172.17.0.0 10101100.00010001. 00000000.00000000 Netmask: 255.255.0.0 = 16 11111111.11111111. 00000000.00000000 Wildcard: 0.0.255.255 00000000.00000000. 11111111.11111111 => Network: 172.17.0.0/16 10101100.00010001. 00000000.00000000 HostMin: 172.17.0.1 10101100.00010001. 00000000.00000001 HostMax: 172.17.255.254 10101100.00010001. 11111111.11111110 Broadcast: 172.17.255.255 10101100.00010001. 11111111.11111111 Hosts/Net: 65534 Class B, Private Internet Subnet Secondary Address Range (Services) - 172.18.0.0/20 Address: 172.18.0.0 10101100.00010010.0000 0000.00000000 Netmask: 255.255.240.0 = 20 11111111.11111111.1111 0000.00000000 Wildcard: 0.0.15.255 00000000.00000000.0000 1111.11111111 => Network: 172.18.0.0/20 10101100.00010010.0000 0000.00000000 HostMin: 172.18.0.1 10101100.00010010.0000 0000.00000001 HostMax: 172.18.15.254 10101100.00010010.0000 1111.11111110 Broadcast: 172.18.15.255 10101100.00010010.0000 1111.11111111 Hosts/Net: 4094 Class B, Private Internet Running the following command will create the VPN on GCP and configure the site-to-site VPN using strongswan using IPSec . make create-tunnel The successful Gateway and Tunnel creation should show the following resources in the GCP console, Cloud VPN Tunnel Cloud VPN Tunnel creates a VPN tunnel between your on-premise(home) network and Google Cloud. As you notice the tunnel\u2019s status is First handshake as we are yet to initate the connection from your VM which we will be doing in the next section. VPN Gateway The VPN gateway helps routing the VPN traffic from our on-premise network into Google Cloud,","title":"IP Plan"},{"location":"site-to-site-vpn/#connect-vm-to-google-cloud","text":"The setup done in the previous step would have enabled the strongswan systemd service on the vm let us check the status of the same, Open a new terminal and ssh into the Vagrant vm cd $TUTORIAL_HOME All the commands in this chapter will be executed inside the VM, so lets SSH into it, vagrant ssh","title":"Connect VM to Google Cloud"},{"location":"site-to-site-vpn/#ipsec-service","text":"The make create-tunnel command would have enabled and started a systemd service called strongswan . Run the following command to check the status of the strongswan service, sudo systemctl status strongswan \u25cf strongswan.service - strongSwan IPsec IKEv1/IKEv2 daemon using swanctl Loaded: loaded ( /lib/systemd/system/strongswan.service ; enabled ; vendor preset: enabled ) Active: active ( running ) since Fri 2021 -10-01 13 :26:48 UTC ; 2s ago Process: 63691 ExecStartPost = /usr/sbin/swanctl --load-all --noprompt ( code = exited, status = 0 /SUCCESS ) Main PID: 63658 ( charon-systemd ) Status: \"charon-systemd running, strongSwan 5.8.2, Linux 5.4.0-88-generic, x86_64\" Tasks: 17 ( limit: 4682 ) Memory: 3 .1M CGroup: /system.slice/strongswan.service \u2514\u250063658 /usr/sbin/charon-systemd Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : selected peer config 'gw-gw' Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : authentication of 'xx.xxx.xx.xxx' with pre-shared key successful Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : authentication of 'xx.xx.xxx.xx' ( myself ) with pre-shared key Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : IKE_SA gw-gw [ 3 ] established between 192 .168.68.119 [ xx.xx.xxx.xx ] ...xx.xxx.xx.xxx [ xx.xxx.xx.xxx ] Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : scheduling reauthentication in 10593s Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : maximum IKE_SA lifetime 11673s Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : selected proposal: ESP:AES_GCM_16_256/NO_EXT_SEQ Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : CHILD_SA home-gcp { 2 } established with SPIs ceb36c53_i c82d0bb7_o and TS 192 .168.0.0/16 === 172 .16.0.0/28 172 .> Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : generating IKE_AUTH response 1 [ IDr AUTH SA TSi TSr N ( AUTH_LFT ) ] Oct 01 13 :26:50 ubuntu-focal charon-systemd [ 63658 ] : sending packet: from 192 .168.68.119 [ 4500 ] to xx.xxx.xx.xxx [ 4500 ] ( 269 bytes ) The service status shows the connection has been successfully loaded, but not established yet.","title":"IPSec Service"},{"location":"site-to-site-vpn/#ipsec-service-configuration","text":"The strongswan IPSec tunnel connection configuration done using the file /etc/swanctl/conf.d/gcp-gloo-demos.conf . For more details about the configuration please check the strongswan docs . connections { gw -gw { local_addrs = 192.168.68.119 remote_addrs = xx.xxx.xxx.xxx local { auth = psk } remote { auth = psk } children { home -gcp { local_ts = 192.168.0.0/16 remote_ts = 172.16.0.0/28 ,172.17.0.0/16,172.18.0.0/20 rekey_time = 5400 rekey_bytes = 500000000 rekey_packets = 1000000 esp_proposals = aes256gcm16-sha512-modp8192 start_action = start dpd_action = restart } } version = 2 mobike = no reauth_time = 10800 proposals = aes256gcm16-sha512-modp4096 } } secrets { ike -1 { id -1 = 192.168.68.119 id -2 = xx.xxx.xxx.xxx secret = <Generated IPSec Secret> } } Points to Ponder The remote_ts is mapped to the GKE k8s subnets that we had defined for Nodes, Pods and Service respectively. This will enable the VM to route the traffic to those subnets via the VPN gateway and VPN tunnel Its very important to have the Cipher 1 proposals and esp_proposals inline with whats supported by GCP IKE Ciphers 2 . Check the names connections \u2192 gw-gw , children \u2192 home-gcp and IKEV2 3 secret \u2192 ike-1 from the gcp-gloo-demos.conf configuration to their statuses from the strongswan service logs. For more detailed logs use sudo journalctl -xe -u strongswan","title":"IPSec Service Configuration"},{"location":"site-to-site-vpn/#check-status","text":"The status of tunnel can be checked on the VM using the swanctl utility, sudo swantctl --list-sas The command above lists the SA as shown, vagrant@ubuntu-focal:~$ sudo swanctl --list-sas gw-gw: #3, ESTABLISHED, IKEv2, 63c3d5bc28f7d6eb_i a80e425a5246e759_r* local 'xx.xxx.xx.xx' @ 192.168.68.119[4500] remote 'xx.xxx.xx.xx' @ xx.xxx.xx.xx[4500] AES_GCM_16-256/PRF_HMAC_SHA2_512/MODP_4096 established 194s ago, reauth in 9809s home-gcp: #2, reqid 1, INSTALLED, TUNNEL-in-UDP, ESP:AES_GCM_16-256 installed 194s ago, rekeying in 4814s, expires in 5746s in c23db81e, 1205 bytes, 6 packets, 119s ago out b4a96c26, 531 bytes, 8 packets, 119s ago local 192.168.0.0/16 remote 172.16.0.0/28 172.17.0.0/16 172.18.0.0/20 When checking the Google Cloud Console it should show the VPN Tunnel status as Established ,","title":"Check Status"},{"location":"site-to-site-vpn/#check-connectivity","text":"Let us check the connections between GKE and VM, kubectl --context = gke apply -f $TUTORIAL_HOME /extras/checks-and-tests.yaml Ping from the pod, kubectl --context = gke exec -it $( kubectl --context = gke get pods -lapp = network-utils -ojsonpath = '{.items[*].metadata.name}' ) -- ping <your VM public ip> The ping should successfull with the output like, # e.g kubectl --context=gke exec -it $(kubectl --context=gke get pods -lapp=network-utils -ojsonpath='{.items[*].metadata.name}') -- ping 192.168.68.119 PING 192 .168.68.119 ( 192 .168.68.119 ) 56 ( 84 ) bytes of data. 64 bytes from 192 .168.68.119: icmp_seq = 1 ttl = 62 time = 32 .6 ms 64 bytes from 192 .168.68.119: icmp_seq = 2 ttl = 62 time = 34 .4 ms 64 bytes from 192 .168.68.119: icmp_seq = 3 ttl = 62 time = 117 ms If tunnel was established successfully the following curl should return an HTML response as shown, curl $( kubectl --context = gke get pods -lapp = nginx -ojsonpath = '{.items[*].status.podIP}' ) <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > < style > html { color-scheme : light dark ; } body { width : 35 em ; margin : 0 auto ; font-family : Tahoma , Verdana , Arial , sans-serif ; } </ style > </ head > < body > < h1 > Welcome to nginx! </ h1 > < p > If you see this page, the nginx web server is successfully installed and working. Further configuration is required. </ p > < p > For online documentation and support please refer to < a href = \"http://nginx.org/\" > nginx.org </ a > . < br /> Commercial support is available at < a href = \"http://nginx.com/\" > nginx.com </ a > . </ p > < p >< em > Thank you for using nginx. </ em ></ p > </ body > </ html > When Tunnel is inactive The VPN tunnel goes inactive if there is no activity for sometime, whenever you see the Tunnel is inactive say you are not able to ping the Pods, try restarting the strongswan service and initiate the connection, vagrant ssh -c \"sudo systemctl restart strongswan\" https://wiki.strongswan.org/projects/strongswan/wiki/IKEv2CipherSuites \u21a9 https://cloud.google.com/network-connectivity/docs/vpn/concepts/supported-ike-ciphers \u21a9 https://en.wikipedia.org/wiki/Internet_Key_Exchange \u21a9","title":"Check Connectivity"},{"location":"tools-and-sources/","text":"At the end of this chapter you will have the required tools and enviroment ready for running the demo. Pre-requsites \u00b6 Access to Google Cloud with Service Account that can create/update/delete, VPC VPN Compute Kubernetes Clusters Access to Google Cloud AWS with permissions to create/update/delete, VPC VPN Compute Kubernetes Clusters CIVO Cloud Account with Access Token, if you dont have you can sign up . Download Tools \u00b6 We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows helm brew install helm Install choco install kubernetes-helm yq v4 brew install yq Download Download jq brew install jq Install choco install yq pipx brew install pipx && pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath kubectl brew install kubectl Download choco install kubernetes-cli kustomize brew install kustomize Download choco install kustomize stern brew install stern Download Download vagrant brew install vagrant Download Download Ansible Install Install N.A Important You will need Gloo Mesh Enterprise License Key to run the demo exercises. If you dont have one, get a trial license from solo.io . Demo Sources \u00b6 Clone the demo sources from the GitHub respository, git clone https://github.com/kameshsampath/gloo-bgc-demo cd gloo-bgc-demo For convinience, we will refer the clone demo sources folder as $TUTORIAL_HOME , export TUTORIAL_HOME = \" $PWD \" Navigate to the project home, cd $TUTORIAL_HOME Ensure Poetry \u00b6 The project uses poetry to setup Python3 virtual environment to run ansible scripts. You can use pipx to install poetry like, pipx install poetry && pipx ensurepath Lets ensure poetry is setup correctly, running the following command should return the version of the installed poetry utility. poetry --version Let us configure poetry to create the python3 virutalenv in the project directory $TUTORIAL_HOME , poetry config virtualenvs.in-project true To create the virutal environment run the following command, poetry install The command will instal all the required python modules in the $TUTORIAL_HOME/.venv . Install the ansible roles and collections that will be used by the playbooks, poetry run ansible-galaxy role install -r $TUTORIAL_HOME /requirements.yml poetry run ansible-galaxy collection install -r $TUTORIAL_HOME /requirements.yml Tip poetry run is shortcut to invoke the python3 modules from your virtualenv","title":"Tools and Sources"},{"location":"tools-and-sources/#pre-requsites","text":"Access to Google Cloud with Service Account that can create/update/delete, VPC VPN Compute Kubernetes Clusters Access to Google Cloud AWS with permissions to create/update/delete, VPC VPN Compute Kubernetes Clusters CIVO Cloud Account with Access Token, if you dont have you can sign up .","title":"Pre-requsites"},{"location":"tools-and-sources/#download-tools","text":"We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows helm brew install helm Install choco install kubernetes-helm yq v4 brew install yq Download Download jq brew install jq Install choco install yq pipx brew install pipx && pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath kubectl brew install kubectl Download choco install kubernetes-cli kustomize brew install kustomize Download choco install kustomize stern brew install stern Download Download vagrant brew install vagrant Download Download Ansible Install Install N.A Important You will need Gloo Mesh Enterprise License Key to run the demo exercises. If you dont have one, get a trial license from solo.io .","title":"Download Tools"},{"location":"tools-and-sources/#demo-sources","text":"Clone the demo sources from the GitHub respository, git clone https://github.com/kameshsampath/gloo-bgc-demo cd gloo-bgc-demo For convinience, we will refer the clone demo sources folder as $TUTORIAL_HOME , export TUTORIAL_HOME = \" $PWD \" Navigate to the project home, cd $TUTORIAL_HOME","title":"Demo Sources"},{"location":"tools-and-sources/#ensure-poetry","text":"The project uses poetry to setup Python3 virtual environment to run ansible scripts. You can use pipx to install poetry like, pipx install poetry && pipx ensurepath Lets ensure poetry is setup correctly, running the following command should return the version of the installed poetry utility. poetry --version Let us configure poetry to create the python3 virutalenv in the project directory $TUTORIAL_HOME , poetry config virtualenvs.in-project true To create the virutal environment run the following command, poetry install The command will instal all the required python modules in the $TUTORIAL_HOME/.venv . Install the ansible roles and collections that will be used by the playbooks, poetry run ansible-galaxy role install -r $TUTORIAL_HOME /requirements.yml poetry run ansible-galaxy collection install -r $TUTORIAL_HOME /requirements.yml Tip poetry run is shortcut to invoke the python3 modules from your virtualenv","title":"Ensure Poetry"},{"location":"traffic/","text":"At the end of this chapter you would have, Applied Access Policies Create TrafficPolicy to distribute traffic Integrated VM Workload with Mesh Ensure Environment \u00b6 Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc Delete Existing Test Gateway \u00b6 As we will be configuring the services to use the Gloo Mesh VirtualGateway we will delete the existing Istio test gateways from both the workload clusterst, kubectl --context = \" ${ CLUSTER1 } \" delete \\ -n blue-green-canary \\ -k \" $TUTORIAL_HOME /demo-app/config/istio\" kubectl --context = \" ${ CLUSTER2 } \" delete \\ -n blue-green-canary \\ -k \" $TUTORIAL_HOME /demo-app/config/istio\" Let us query the VirtualService( vs ) and Gateways( gw ) from the workload clusters and check our stocks, kubectl --context = \" ${ CLUSTER1 } \" get vs,gw -n istio-system NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h NAME AGE gateway.networking.istio.io/cross-network-gateway 35h gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 14h gateway.networking.istio.io/istiod-gateway 35h kubectl --context = \" ${ CLUSTER2 } \" get vs,gw -n istio-system NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h NAME AGE gateway.networking.istio.io/cross-network-gateway 35h gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster2-gloo-mesh 14h gateway.networking.istio.io/istiod-gateway 35h Gloo Mesh Gateway \u00b6 Let us now create VirtualGateway , VirtualHost and RouteTable to route to enable traffic routing to the blue-green-canary service across the VirtualMesh . Note The VirtualMesh includes the on-premise VM Deploy Virtual Gateway \u00b6 $TUTORIAL_HOME /bin/7_deploy_gateway.sh Let us verify if Gateway and VirtualService are created on both the workload clusters, CLUSTER1 \u00b6 kubectl --context = ${ CLUSTER1 } get gw,vs -A NAMESPACE NAME AGE istio-system gateway.networking.istio.io/bgc-virtualgateway-17072781039916753854 5s istio-system gateway.networking.istio.io/cross-network-gateway 35h istio-system gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 14h istio-system gateway.networking.istio.io/istiod-gateway 35h NAMESPACE NAME GATEWAYS HOSTS AGE gloo-mesh virtualservice.networking.istio.io/bgc-virtualhost-gloo-mesh [\"istio-system/bgc-virtualgateway-17072781039916753854\"] [\"*\"] 5s istio-system virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h CLUSTER2 \u00b6 kubectl --context = ${ CLUSTER2 } get gw,vs -A NAMESPACE NAME AGE istio-system gateway.networking.istio.io/bgc-virtualgateway-9692221184781295762 47s istio-system gateway.networking.istio.io/cross-network-gateway 35h istio-system gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster2-gloo-mesh 14h istio-system gateway.networking.istio.io/istiod-gateway 35h NAMESPACE NAME GATEWAYS HOSTS AGE gloo-mesh virtualservice.networking.istio.io/bgc-virtualhost-gloo-mesh [\"istio-system/bgc-virtualgateway-9692221184781295762\"] [\"*\"] 47s istio-system virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h Calling Service \u00b6 Cluster1 \u00b6 Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Call Service \u00b6 Call the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" Poll Service \u00b6 Poll the service using the script, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" Use Browser \u00b6 Open the URL in the browser open http://$SVC_GW_CLUSTER1 . Cluster2 \u00b6 Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER2 = $( kubectl --context ${ CLUSTER2 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Call Service \u00b6 Call the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \" Poll Service \u00b6 Poll the service using the script, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER2 } \" Use Browser \u00b6 Open the URL in the browser open http://$SVC_GW_CLUSTER2 . As you have observed the blue-green-canary service is by default configured to return the response from version blue . Traffic Policy \u00b6 As we have unified the mesh we are good to distribute traffic amongst them. As part of the next section we will apply various traffic policies to distribute traffic amongst the blue , green and canary services. Before we try the traffic shifting, open the service in the browser, $TUTORIAL_HOME /bin/browse_bgc_service.sh \" ${ CLUSTER1 } \" Green \u00b6 As we already have traffic sent to blue , let use try sending all the traffic to green $TUTORIAL_HOME /bin/8_green.sh Now if you try to call the service via browser or cli as described it should return response from green service. Canary \u00b6 Let us now try sending all the traffic to canary service on the VM, $TUTORIAL_HOME /bin/9_canary.sh Now if you try to call the service via browser or cli as described it should return response from canary service that is deployed in on-premise VM. Blue \u2190 \u2192 Green \u00b6 Let\u2019s try to split the traffic between blue ( 50% ) and green ( 50% ), $TUTORIAL_HOME /bin/11_blue-green.sh If you try check your browser you should see an alternating blue-green traffic. Blue,Green and Canary \u00b6 Finally let\u2019s try to split the traffic between blue ( 40% ), green ( 40% ) and canary ( 20% ), $TUTORIAL_HOME /bin/12_blue-green-canary.sh If you try check your browser you should see almost equal traffic to blue and green and few requests to canary . Now we checked the traffic distribution amongst revsions that are deployed on Istio clusters and VM. In the next chapter lets us apply access policies to restrict who and from where we can acess the service.","title":"Traffic Management"},{"location":"traffic/#ensure-environment","text":"Navigate to Tutorial home cd $TUTORIAL_HOME Set cluster environment variables source $TUTORIAL_HOME /.envrc","title":"Ensure Environment"},{"location":"traffic/#delete-existing-test-gateway","text":"As we will be configuring the services to use the Gloo Mesh VirtualGateway we will delete the existing Istio test gateways from both the workload clusterst, kubectl --context = \" ${ CLUSTER1 } \" delete \\ -n blue-green-canary \\ -k \" $TUTORIAL_HOME /demo-app/config/istio\" kubectl --context = \" ${ CLUSTER2 } \" delete \\ -n blue-green-canary \\ -k \" $TUTORIAL_HOME /demo-app/config/istio\" Let us query the VirtualService( vs ) and Gateways( gw ) from the workload clusters and check our stocks, kubectl --context = \" ${ CLUSTER1 } \" get vs,gw -n istio-system NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h NAME AGE gateway.networking.istio.io/cross-network-gateway 35h gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 14h gateway.networking.istio.io/istiod-gateway 35h kubectl --context = \" ${ CLUSTER2 } \" get vs,gw -n istio-system NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h NAME AGE gateway.networking.istio.io/cross-network-gateway 35h gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster2-gloo-mesh 14h gateway.networking.istio.io/istiod-gateway 35h","title":"Delete Existing Test Gateway"},{"location":"traffic/#gloo-mesh-gateway","text":"Let us now create VirtualGateway , VirtualHost and RouteTable to route to enable traffic routing to the blue-green-canary service across the VirtualMesh . Note The VirtualMesh includes the on-premise VM","title":"Gloo Mesh Gateway"},{"location":"traffic/#deploy-virtual-gateway","text":"$TUTORIAL_HOME /bin/7_deploy_gateway.sh Let us verify if Gateway and VirtualService are created on both the workload clusters,","title":"Deploy Virtual Gateway"},{"location":"traffic/#cluster1","text":"kubectl --context = ${ CLUSTER1 } get gw,vs -A NAMESPACE NAME AGE istio-system gateway.networking.istio.io/bgc-virtualgateway-17072781039916753854 5s istio-system gateway.networking.istio.io/cross-network-gateway 35h istio-system gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 14h istio-system gateway.networking.istio.io/istiod-gateway 35h NAMESPACE NAME GATEWAYS HOSTS AGE gloo-mesh virtualservice.networking.istio.io/bgc-virtualhost-gloo-mesh [\"istio-system/bgc-virtualgateway-17072781039916753854\"] [\"*\"] 5s istio-system virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h","title":"CLUSTER1"},{"location":"traffic/#cluster2","text":"kubectl --context = ${ CLUSTER2 } get gw,vs -A NAMESPACE NAME AGE istio-system gateway.networking.istio.io/bgc-virtualgateway-9692221184781295762 47s istio-system gateway.networking.istio.io/cross-network-gateway 35h istio-system gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster2-gloo-mesh 14h istio-system gateway.networking.istio.io/istiod-gateway 35h NAMESPACE NAME GATEWAYS HOSTS AGE gloo-mesh virtualservice.networking.istio.io/bgc-virtualhost-gloo-mesh [\"istio-system/bgc-virtualgateway-9692221184781295762\"] [\"*\"] 47s istio-system virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 35h","title":"CLUSTER2"},{"location":"traffic/#calling-service","text":"","title":"Calling Service"},{"location":"traffic/#cluster1_1","text":"Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' )","title":"Cluster1"},{"location":"traffic/#call-service","text":"Call the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \"","title":"Call Service"},{"location":"traffic/#poll-service","text":"Poll the service using the script, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \"","title":"Poll Service"},{"location":"traffic/#use-browser","text":"Open the URL in the browser open http://$SVC_GW_CLUSTER1 .","title":"Use Browser"},{"location":"traffic/#cluster2_1","text":"Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER2 = $( kubectl --context ${ CLUSTER2 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' )","title":"Cluster2"},{"location":"traffic/#call-service_1","text":"Call the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \"","title":"Call Service"},{"location":"traffic/#poll-service_1","text":"Poll the service using the script, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER2 } \"","title":"Poll Service"},{"location":"traffic/#use-browser_1","text":"Open the URL in the browser open http://$SVC_GW_CLUSTER2 . As you have observed the blue-green-canary service is by default configured to return the response from version blue .","title":"Use Browser"},{"location":"traffic/#traffic-policy","text":"As we have unified the mesh we are good to distribute traffic amongst them. As part of the next section we will apply various traffic policies to distribute traffic amongst the blue , green and canary services. Before we try the traffic shifting, open the service in the browser, $TUTORIAL_HOME /bin/browse_bgc_service.sh \" ${ CLUSTER1 } \"","title":"Traffic Policy"},{"location":"traffic/#green","text":"As we already have traffic sent to blue , let use try sending all the traffic to green $TUTORIAL_HOME /bin/8_green.sh Now if you try to call the service via browser or cli as described it should return response from green service.","title":"Green"},{"location":"traffic/#canary","text":"Let us now try sending all the traffic to canary service on the VM, $TUTORIAL_HOME /bin/9_canary.sh Now if you try to call the service via browser or cli as described it should return response from canary service that is deployed in on-premise VM.","title":"Canary"},{"location":"traffic/#blue-green","text":"Let\u2019s try to split the traffic between blue ( 50% ) and green ( 50% ), $TUTORIAL_HOME /bin/11_blue-green.sh If you try check your browser you should see an alternating blue-green traffic.","title":"Blue &lt;-- --&gt; Green"},{"location":"traffic/#bluegreen-and-canary","text":"Finally let\u2019s try to split the traffic between blue ( 40% ), green ( 40% ) and canary ( 20% ), $TUTORIAL_HOME /bin/12_blue-green-canary.sh If you try check your browser you should see almost equal traffic to blue and green and few requests to canary . Now we checked the traffic distribution amongst revsions that are deployed on Istio clusters and VM. In the next chapter lets us apply access policies to restrict who and from where we can acess the service.","title":"Blue,Green and Canary"},{"location":"troubleshooting/","text":"Tunnel Closed \u00b6 The VPN tunnel goes inactive if there is no activity, whenever you see the Tunnel is inactive say you are not able to ping the Pods, try restarting the strongswan service and initiate the connection, vagrant ssh -c \"sudo systemctl restart strongswan\" vagrant ssh -c \"sudo swanctl initiate --child=home-gcp\"","title":"Troubleshooting"},{"location":"troubleshooting/#tunnel-closed","text":"The VPN tunnel goes inactive if there is no activity, whenever you see the Tunnel is inactive say you are not able to ping the Pods, try restarting the strongswan service and initiate the connection, vagrant ssh -c \"sudo systemctl restart strongswan\" vagrant ssh -c \"sudo swanctl initiate --child=home-gcp\"","title":"Tunnel Closed"}]}