{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"As part of this short tutorial/demo we will be deploying a simple colored hello world microservice and explore how to integrate the service running on an on-premise VM with public cloud clusters using Istio. We will also leverage Gloo Mesh federation capabilites to unify the Istio clusters to provide better interoprablity between the clusters and VM. What we wil be doing as part of this demo ? \u00b6 Setup Kubernetes clusters on three public clouds Setup Virutal Machine on on-premise network Site-to-Site VPN to connect on-premise to public cloud Deploy Gloo Mesh Enterprise on clusters Deploy Istio on clusters Deploy Istio sidecar on VM Traffic Distribution between VM on-premise and public cloud Access Policies to control traffic","title":"Overview"},{"location":"#what-we-wil-be-doing-as-part-of-this-demo","text":"Setup Kubernetes clusters on three public clouds Setup Virutal Machine on on-premise network Site-to-Site VPN to connect on-premise to public cloud Deploy Gloo Mesh Enterprise on clusters Deploy Istio on clusters Deploy Istio sidecar on VM Traffic Distribution between VM on-premise and public cloud Access Policies to control traffic","title":"What we wil be doing as part of this demo ?"},{"location":"access-policies/","text":"In this chapter we will create and apply access policies that will restrict access to services in Gloo Mesh Clusters and VM. Ensure environment \u00b6 Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config Check if you are still in the work/mgmt folder, cd $TUTORIAL_HOME /work/mgmt Enable Global Access Policy \u00b6 The Virtual Mesh that we crated in earlier as the globalAccessPolicy disabled, kubectl get virtualmeshes.networking.mesh.gloo.solo.io -n gloo-mesh bgc-virtual-mesh -o json | jq '.spec.globalAccessPolicy' The command should return DISABLED Now lets enable the globalAccessPolicy , $TUTORIAL_HOME /bin/17_toggle_mesh_access_policy.sh Lets verify if we are able to access the service, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" The command should show an output like: ################################################### Polling Service URL 35.244.63.221/api ################################################### RBAC: access denied By default the moment we enable globalAccessPolicy the Gloo Mesh create allowNone with out any access policy associated with the resource, hence you the RBAC: access denied as output when calling the service. Enable Ingress Traffic via Istio \u00b6 Let us now allow the traffic via Istio Ingress Gateway, $TUTORIAL_HOME /bin/18_enable_access_via_ingress.sh Let us try polling the service, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should show the output from the service: ################################################### Polling Service URL 35 .244.63.221/api ################################################### { \"color\" : \"blue\" , \"count\" : 476 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 477 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } RBAC: access denied { \"color\" : \"green\" , \"count\" : 2173 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"green\" , \"count\" : 2174 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } As you noticed the command shows one RBAC: access denied , that is from the VM Canary service. What we did right now is allowing traffic from the Gloo Mesh Clusters and the traffic from and to vm is still blocked. Allow Traffic to VM \u00b6 Let us now apply the Access policy that will allow the traffic to the VM, Let us now allow the traffic via Istio Ingress Gateway, $TUTORIAL_HOME /bin/19_enable_access_to_vm.sh Let us try polling the service, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should now include the output from the Canary(VM) service as well, ################################################### Polling Service URL 35 .244.63.221/api ################################################### { \"color\" : \"green\" , \"count\" : 2175 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 478 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } ... { \"color\" : \"yellow\" , \"count\" : 197 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.114\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" } Access service from VM \u00b6 Let us now check if we are able to access the service from the VM, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" As expected we got RBAC: access denied as we are yet to allow the traffic from the VM to the cluster, HTTP/1.1 403 Forbidden content-length: 19 content-type: text/plain date: Thu, 23 Sep 2021 16:29:22 GMT server: envoy x-envoy-upstream-service-time: 269 RBAC: access denied Let us allow the VM to access the services on the Cluster, $TUTORIAL_HOME /bin/20_enable_access_from_vm.sh Now trying again to access the service from VM should show a successful response, $ vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" HTTP/1.1 200 OK content-length: 129 content-type: application/json ; charset = utf-8 date: Thu, 23 Sep 2021 16 :32:22 GMT etag: \"81-44uxFMXdXGKizzXNw4AXYnD7xj0\" server: envoy vary: Accept-Encoding x-envoy-upstream-service-time: 285 { \"color\" : \"yellow\" , \"count\" : 198 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.114\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" } Voli\u00e0! We have successfully enabled the Access Policies on our Gloo Mesh cluster.","title":"Access Policies"},{"location":"access-policies/#ensure-environment","text":"Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config Check if you are still in the work/mgmt folder, cd $TUTORIAL_HOME /work/mgmt","title":"Ensure environment"},{"location":"access-policies/#enable-global-access-policy","text":"The Virtual Mesh that we crated in earlier as the globalAccessPolicy disabled, kubectl get virtualmeshes.networking.mesh.gloo.solo.io -n gloo-mesh bgc-virtual-mesh -o json | jq '.spec.globalAccessPolicy' The command should return DISABLED Now lets enable the globalAccessPolicy , $TUTORIAL_HOME /bin/17_toggle_mesh_access_policy.sh Lets verify if we are able to access the service, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" The command should show an output like: ################################################### Polling Service URL 35.244.63.221/api ################################################### RBAC: access denied By default the moment we enable globalAccessPolicy the Gloo Mesh create allowNone with out any access policy associated with the resource, hence you the RBAC: access denied as output when calling the service.","title":"Enable Global Access Policy"},{"location":"access-policies/#enable-ingress-traffic-via-istio","text":"Let us now allow the traffic via Istio Ingress Gateway, $TUTORIAL_HOME /bin/18_enable_access_via_ingress.sh Let us try polling the service, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should show the output from the service: ################################################### Polling Service URL 35 .244.63.221/api ################################################### { \"color\" : \"blue\" , \"count\" : 476 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 477 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } RBAC: access denied { \"color\" : \"green\" , \"count\" : 2173 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"green\" , \"count\" : 2174 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } As you noticed the command shows one RBAC: access denied , that is from the VM Canary service. What we did right now is allowing traffic from the Gloo Mesh Clusters and the traffic from and to vm is still blocked.","title":"Enable Ingress Traffic via Istio"},{"location":"access-policies/#allow-traffic-to-vm","text":"Let us now apply the Access policy that will allow the traffic to the VM, Let us now allow the traffic via Istio Ingress Gateway, $TUTORIAL_HOME /bin/19_enable_access_to_vm.sh Let us try polling the service, $TUTORIAL_HOME /bin/poll_bgc_service.sh \" ${ CLUSTER1 } \" The command should now include the output from the Canary(VM) service as well, ################################################### Polling Service URL 35 .244.63.221/api ################################################### { \"color\" : \"green\" , \"count\" : 2175 , \"greeting\" : \"Bonjour \ud83d\udc4b\ud83c\udffd\" , \"pod\" : \"green-7d9f5f4b47-rcb76\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } { \"color\" : \"blue\" , \"count\" : 478 , \"greeting\" : \"Namaste \ud83d\ude4f\ud83c\udffd\" , \"pod\" : \"blue-98db67777-797t7\" , \"textColor\" : \"whitesmoke\" , \"userAgent\" : \"HTTPie/2.5.0\" } ... { \"color\" : \"yellow\" , \"count\" : 197 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.114\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" }","title":"Allow Traffic to VM"},{"location":"access-policies/#access-service-from-vm","text":"Let us now check if we are able to access the service from the VM, vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" As expected we got RBAC: access denied as we are yet to allow the traffic from the VM to the cluster, HTTP/1.1 403 Forbidden content-length: 19 content-type: text/plain date: Thu, 23 Sep 2021 16:29:22 GMT server: envoy x-envoy-upstream-service-time: 269 RBAC: access denied Let us allow the VM to access the services on the Cluster, $TUTORIAL_HOME /bin/20_enable_access_from_vm.sh Now trying again to access the service from VM should show a successful response, $ vagrant ssh -c \"http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api\" HTTP/1.1 200 OK content-length: 129 content-type: application/json ; charset = utf-8 date: Thu, 23 Sep 2021 16 :32:22 GMT etag: \"81-44uxFMXdXGKizzXNw4AXYnD7xj0\" server: envoy vary: Accept-Encoding x-envoy-upstream-service-time: 285 { \"color\" : \"yellow\" , \"count\" : 198 , \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"pod\" : \"vm-192.168.68.114\" , \"textColor\" : \"black\" , \"userAgent\" : \"HTTPie/2.5.0\" } Voli\u00e0! We have successfully enabled the Access Policies on our Gloo Mesh cluster.","title":"Access service from VM"},{"location":"deploy-blue/","text":"Ensure environment \u00b6 cd $TUTORIAL_HOME /work/mgmt Deploy Blue Application \u00b6 $TUTORIAL_HOME /bin/2_deploy_blue.sh \" ${ CLUSTER1 } \" Deploy httpbin \u00b6 The httpin application will be used to for checking various features of Gloo Mesh that you will be done in upcoming sections, kubectl --context = \" ${ CLUSTER1 } \" apply -f $TUTORIAL_HOME /extras/httpbin.yaml Create Gateway and Virtual Service \u00b6 We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER1 } \" Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER1 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \"","title":"Deploy Blue"},{"location":"deploy-blue/#ensure-environment","text":"cd $TUTORIAL_HOME /work/mgmt","title":"Ensure environment"},{"location":"deploy-blue/#deploy-blue-application","text":"$TUTORIAL_HOME /bin/2_deploy_blue.sh \" ${ CLUSTER1 } \"","title":"Deploy Blue Application"},{"location":"deploy-blue/#deploy-httpbin","text":"The httpin application will be used to for checking various features of Gloo Mesh that you will be done in upcoming sections, kubectl --context = \" ${ CLUSTER1 } \" apply -f $TUTORIAL_HOME /extras/httpbin.yaml","title":"Deploy httpbin"},{"location":"deploy-blue/#create-gateway-and-virtual-service","text":"We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER1 } \" Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER1 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \"","title":"Create Gateway and Virtual Service"},{"location":"deploy-canary/","text":"At the end of this chapter you would have, Deployed Canary App on VM Create VM resources Configure Isito Sidecar Istio Control Plane resources Ensure environment \u00b6 Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config cd $TUTORIAL_HOME /work/mgmt Prepare Istio to Integrate VM Resources \u00b6 For this demo we will make Cluster-1 host the VM resources. For the VM integration we need to have istiod exposed via gateway. The following sections deploys a set of Istio resources that will allow us to onboard VMs to Istio Servicemesh via Cluster-1 . kubectl --context = $CLUSTER1 apply -k $TUTORIAL_HOME /cluster/vm/istio Onboard VM Workloads \u00b6 The VM workloads will be hosted on a namespace called $VM_NAMESPACE in $CLUSTER1 . The VM workloads will be run using a Kubernetes SA called $SERVICE_ACCOUNT . There will be set of Istio resources that will be generated which need to be added to VM and configured to be used by Istio sidecar in the VM. Before we create the necessary resources, let us set some environment variables for our convinience, TODO update to be ansible variables export VM_APP = \"blue-green-canary\" export VM_NAMESPACE = \"vm-blue-green-canary\" export WORK_DIR = \" $TUTORIAL_HOME /cloud/private/vm\" export SERVICE_ACCOUNT = \"blue-green-canary\" export CLUSTER_NETWORK = \"bgc-network1\" export VM_NETWORK = \"bgc-vm-network\" export CLUSTER = \"cluster1\" Create the Namespace and SA, kubectl --context = $CLUSTER1 apply -k $TUTORIAL_HOME /cluster/vm/workload Integrate VM with Istio \u00b6 We now need create the isito resourceson VM and start the Istio sidecar service, we can do that by running the follwing command. The command will run set of Ansible Tasks that will setup and configure Istio sidecar on the VM. make workload-run Create VM Mesh Resources \u00b6 On Management Cluster, envsubst < $TUTORIAL_HOME /mesh-files/vm/workload-destination.yaml | kubectl --context = $MGMT apply -f - On Cluster-1, envsubst < $TUTORIAL_HOME /mesh-files/vm/service-workload-entry.yaml | kubectl --context = $CLUSTER1 apply -f - Access Policies \u00b6 Important Currently we cant route traffic to VMs, hence traffic distribution from the browser will still be only between Kubernetes services Placeholder to check TP with VM when its avaiable From VM \u00b6 With globalAccessPolicy disabled in the virtual mesh, calling a Istio service from VM should go through successfully, [vagrant@fedora ~]$ http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api HTTP/1.1 200 OK content-length: 137 content-type: application/json; charset=utf-8 date: Thu, 23 Sep 2021 14:06:46 GMT etag: \"89-coTFNpCn3MGCfccrhUII0cS8+hw\" server: envoy vary: Accept-Encoding x-envoy-upstream-service-time: 179 { \"color\": \"blue\", \"count\": 0, \"greeting\": \"Namaste \ud83d\ude4f\ud83c\udffd\", \"pod\": \"blue-98db67777-797t7\", \"textColor\": \"whitesmoke\", \"userAgent\": \"HTTPie/2.5.0\" } To VM \u00b6 We should also be able to access the VM from the cluster as shown, kubectl exec -c network-utils -it $( kubectl get pods -lapp = network-utils --no-headers | awk '{print $1}' ) -- curl blue-green-canary.vm-blue-green-canary.svc.cluster.local:8080/api The command should show an output like, { \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"count\" : 1 , \"pod\" : \"vm-192.168.68.114\" , \"color\" : \"yellow\" , \"textColor\" : \"black\" , \"userAgent\" : \"curl/7.78.0\" }","title":"Deploy Canary"},{"location":"deploy-canary/#ensure-environment","text":"Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config cd $TUTORIAL_HOME /work/mgmt","title":"Ensure environment"},{"location":"deploy-canary/#prepare-istio-to-integrate-vm-resources","text":"For this demo we will make Cluster-1 host the VM resources. For the VM integration we need to have istiod exposed via gateway. The following sections deploys a set of Istio resources that will allow us to onboard VMs to Istio Servicemesh via Cluster-1 . kubectl --context = $CLUSTER1 apply -k $TUTORIAL_HOME /cluster/vm/istio","title":"Prepare Istio to Integrate VM Resources"},{"location":"deploy-canary/#onboard-vm-workloads","text":"The VM workloads will be hosted on a namespace called $VM_NAMESPACE in $CLUSTER1 . The VM workloads will be run using a Kubernetes SA called $SERVICE_ACCOUNT . There will be set of Istio resources that will be generated which need to be added to VM and configured to be used by Istio sidecar in the VM. Before we create the necessary resources, let us set some environment variables for our convinience, TODO update to be ansible variables export VM_APP = \"blue-green-canary\" export VM_NAMESPACE = \"vm-blue-green-canary\" export WORK_DIR = \" $TUTORIAL_HOME /cloud/private/vm\" export SERVICE_ACCOUNT = \"blue-green-canary\" export CLUSTER_NETWORK = \"bgc-network1\" export VM_NETWORK = \"bgc-vm-network\" export CLUSTER = \"cluster1\" Create the Namespace and SA, kubectl --context = $CLUSTER1 apply -k $TUTORIAL_HOME /cluster/vm/workload","title":"Onboard VM Workloads"},{"location":"deploy-canary/#integrate-vm-with-istio","text":"We now need create the isito resourceson VM and start the Istio sidecar service, we can do that by running the follwing command. The command will run set of Ansible Tasks that will setup and configure Istio sidecar on the VM. make workload-run","title":"Integrate VM with Istio"},{"location":"deploy-canary/#create-vm-mesh-resources","text":"On Management Cluster, envsubst < $TUTORIAL_HOME /mesh-files/vm/workload-destination.yaml | kubectl --context = $MGMT apply -f - On Cluster-1, envsubst < $TUTORIAL_HOME /mesh-files/vm/service-workload-entry.yaml | kubectl --context = $CLUSTER1 apply -f -","title":"Create VM Mesh Resources"},{"location":"deploy-canary/#access-policies","text":"Important Currently we cant route traffic to VMs, hence traffic distribution from the browser will still be only between Kubernetes services Placeholder to check TP with VM when its avaiable","title":"Access Policies"},{"location":"deploy-canary/#from-vm","text":"With globalAccessPolicy disabled in the virtual mesh, calling a Istio service from VM should go through successfully, [vagrant@fedora ~]$ http blue-green-canary.blue-green-canary.svc.cluster.local:8080/api HTTP/1.1 200 OK content-length: 137 content-type: application/json; charset=utf-8 date: Thu, 23 Sep 2021 14:06:46 GMT etag: \"89-coTFNpCn3MGCfccrhUII0cS8+hw\" server: envoy vary: Accept-Encoding x-envoy-upstream-service-time: 179 { \"color\": \"blue\", \"count\": 0, \"greeting\": \"Namaste \ud83d\ude4f\ud83c\udffd\", \"pod\": \"blue-98db67777-797t7\", \"textColor\": \"whitesmoke\", \"userAgent\": \"HTTPie/2.5.0\" }","title":"From VM"},{"location":"deploy-canary/#to-vm","text":"We should also be able to access the VM from the cluster as shown, kubectl exec -c network-utils -it $( kubectl get pods -lapp = network-utils --no-headers | awk '{print $1}' ) -- curl blue-green-canary.vm-blue-green-canary.svc.cluster.local:8080/api The command should show an output like, { \"greeting\" : \"Hola \u270b\ud83c\udffd\" , \"count\" : 1 , \"pod\" : \"vm-192.168.68.114\" , \"color\" : \"yellow\" , \"textColor\" : \"black\" , \"userAgent\" : \"curl/7.78.0\" }","title":"To VM"},{"location":"deploy-green/","text":"Navigate to $CLUSTER2 work folder cd $TUTORIAL_HOME /work/mgmt Deploy Green Application \u00b6 $TUTORIAL_HOME /bin/3_deploy_green.sh \" ${ CLUSTER2 } \" Deploy httpbin \u00b6 The httpin application will be used to for checking various features of Gloo Mesh that you will be done in upcoming sections, kubectl --context = \" ${ CLUSTER2 } \" apply -f $TUTORIAL_HOME /extras/httpbin.yaml Create Gateway and Virtual Service \u00b6 We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER2 } \" And now when call or poll the service you should see traffic distributed among the the blue and green, Cluster -1 \u00b6 Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER1 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" Cluster- 2 \u00b6 Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER2 = $( kubectl --context ${ CLUSTER2 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER2 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \"","title":"Deploy Green"},{"location":"deploy-green/#deploy-green-application","text":"$TUTORIAL_HOME /bin/3_deploy_green.sh \" ${ CLUSTER2 } \"","title":"Deploy Green Application"},{"location":"deploy-green/#deploy-httpbin","text":"The httpin application will be used to for checking various features of Gloo Mesh that you will be done in upcoming sections, kubectl --context = \" ${ CLUSTER2 } \" apply -f $TUTORIAL_HOME /extras/httpbin.yaml","title":"Deploy httpbin"},{"location":"deploy-green/#create-gateway-and-virtual-service","text":"We need to create the Gateway and Virtual service for Blue-Green-Canary application. The gateway and virtual service will allow the application to be accessed via Istio Ingress Gateway, $TUTORIAL_HOME /bin/test_gateway.sh \" ${ CLUSTER2 } \" And now when call or poll the service you should see traffic distributed among the the blue and green,","title":"Create Gateway and Virtual Service"},{"location":"deploy-green/#cluster-1","text":"Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER1 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \"","title":"Cluster -1"},{"location":"deploy-green/#cluster-2","text":"Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER2 = $( kubectl --context ${ CLUSTER2 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER2 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \"","title":"Cluster- 2"},{"location":"env-setup/","text":"At the end of this chapter you would have, Three Kubernetes Clusters on AWS,GCP and Civo The VM on local environment Installed Gloo Mesh Enterprise Ensure Enviroment \u00b6 For easier setup and convinience setup the following enviroment variables before starting the setup: # the Tutorial home directory export TUTORIAL_HOME = \" $( realpath -m \" $PWD /../..\" ) \" # this is not available at this point but will be useful later export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config export GOOGLE_APPLICATION_CREDENTIALS = <your google SA JSON FILE> export GOOGLE_PROJECT_ID = <your GCP test project> # The password file where your Ansible valut password will be stored export VAULT_FILE = .password_file # the vagrant vm provider export VAGRANT_DEFAULT_PROVIDER = virtualbox # Vagrant base box ubuntu/focal64, fedora/34-cloud-base export VAGRANT_BOX = ubuntu/focal64 Tip A vary handy tool to work with environment and variables direnv Demo Environment \u00b6 The demo requires us to have three Kubernetes clusters and one Virtual Machine. The following tables shows the environment to component matrix. Components GCP AWS CIVO VM Cluster Name cluster-1 cluster-2 mgmt Gloo Mesh Management Gloo Mesh Agent Kubernetes Istio Blue-Green-Canary Service Navigate to the $TUTORIAL_HOME , cd $TUTORIAL_HOME Create the Ansible variables file, make encrypt-vars Note As we will having sensitive keys in the Ansible variables, its highly recommended to create an encrypted variables file. The following table shows the ansible variables used, do update the values of the file as per your environment. Variable Description Default gcp_vpc_name The name of the VPC that will be created. Same VPC will be used with GKE Cluster kameshs-k8s-vpc gcp_vpn_name The name of the VPN that will be created kameshs-site-to-site-vpn gcp_cred_file The SA JSON file to be copied to VM \u201c$HOME/.config/gcloud/credentials\u201d from the Ansible controller machine gcp_project The GCP project to use gcp_region The GCP region to use asia-south-1 aws_access_key_id The AWS Acess Key aws_secret_access_key The AWS Secret Key aws_region The AWS Region to use ap-south-1 eks_cluster_name The AWS EKS cluster name kameshs-gloo-demos aws_vpc_name The AWS VPC name. The same VPC will be used with EKS cluster kameshs-gloo-demos civo_api_key The CIVO Cloud API Key civo_k3s_cluster_name The CIVO Kubernetes cluster Name gloo-mgmt force_app_install Force install application on VM no clean_istio_vm_files Clean the generated Istio VM files no k8s_context The kubernetes context that will be used to query Istio resources VM Setup \u00b6 We will use Vagrant to run and configure our workload VM. The Workload VM also serves as our Ansible target host to run the ansible playbooks. make vm-up The make vm-up will bring up the vagrant VM with with base packages installed. Cloud Setup \u00b6 The cloud setup will setup the Clouds and create Kubernetes clusters on them. Google Cloud \u00b6 The following command will run setup Google Cloud with VPC, VPN and setup Kubernetes using the VPC. This task will also setup a site-to-site VPN using strongswan which will allow communication between on-premise(VM) to the GKE. make cloud-gcp-run Civo Cloud \u00b6 The following command will setup a Kubernetes Cluster on Civo cloud, make cloud-civo-run Amzon Cloud \u00b6 The following command will setup a Kubernetes Cluster(EKS) on AWS cloud, make cloud-civo-run Note The step above will take approx 30-40 mins to complete Deploy Istio \u00b6 We will deploy istio to all our workload clusters, in our setup AWS and GCP are workload clusters, run the following command to install the istio, cd $TUTORIAL_HOME /work/mgmt Install Istio on cluster-1 : $TUTORIAL_HOME /bin/install_istio.sh \" $CLUSTER1 \" Install Istio on cluster-2 : $TUTORIAL_HOME /bin/install_istio.sh \" $CLUSTER2 \" Ensure Environment \u00b6 Make sure you have the Gloo Mesh Gateway Enterprise License Key before proceeding to install. Export the license key to variable, export GLOO_MESH_GATEWAY_LICENSE_KEY = <your Gloo Mesh Gateway EE License Key> Download meshctl \u00b6 Download and install latest meshctl by running, curl -sL https://run.solo.io/meshctl/install | sh Add meshctl to the system path, export PATH = $HOME /.gloo-mesh/bin: $PATH Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config Navigate to management work folder, cd $TUTORIAL_HOME /work/mgmt Install Gloo Mesh Enterprise \u00b6 Create the gloo-mesh namespace to install Gloo Mesh , $TUTORIAL_HOME /bin/install_mesh.sh Note You can safely ignore the helm warnings It will take few minutes for the Gloo Mesh to be installed and ready Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status \ud83d\udfe1 Gloo Mesh Agents Connectivity Hints: * No registered clusters detected. To register a remote cluster that has a deployed Gloo Mesh agent, add a KubernetesCluster CR. For more info, see: https://docs.solo.io/gloo-mesh/latest/setup/cluster_registration/enterprise_cluster_registration/ Management Configuration --------------------------- 2021-09-14T06:41:10.594255Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources Cluster Registrations \u00b6 To connect our hetregenous Isito clusters, we need to register them with the management cluster. The cluster registration will communicate with Management cluster via an enterprise-agent , we need to pass the enterprise-agent address as part of cluster registrations to enable effective communcation. Register $CLUSTER1 as cluster1 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER1 cluster1 Register $CLUSTER2 as cluster2 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER2 cluster2 Verify Install \u00b6 Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status +----------+------------+-------------------------------+-----------------+ | CLUSTER | REGISTERED | DASHBOARDS AND AGENTS PULLING | AGENTS PUSHING | +----------+------------+-------------------------------+-----------------+ | cluster1 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ | cluster2 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ \ud83d\udfe2 Gloo Mesh Agents Connectivity Management Configuration --------------------------- 2021-09-14T04:56:04.288180Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources Mesh Dashboard \u00b6 We will us the Gloo Mesh Dashboard to verify our registered clusters, kubectl --context = $MGMT port-forward -n gloo-mesh deployment/dashboard 8090 :8090 You can then open the dashboard in your browser which will open a page as shown:","title":"Environment Setup"},{"location":"env-setup/#ensure-enviroment","text":"For easier setup and convinience setup the following enviroment variables before starting the setup: # the Tutorial home directory export TUTORIAL_HOME = \" $( realpath -m \" $PWD /../..\" ) \" # this is not available at this point but will be useful later export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config export GOOGLE_APPLICATION_CREDENTIALS = <your google SA JSON FILE> export GOOGLE_PROJECT_ID = <your GCP test project> # The password file where your Ansible valut password will be stored export VAULT_FILE = .password_file # the vagrant vm provider export VAGRANT_DEFAULT_PROVIDER = virtualbox # Vagrant base box ubuntu/focal64, fedora/34-cloud-base export VAGRANT_BOX = ubuntu/focal64 Tip A vary handy tool to work with environment and variables direnv","title":"Ensure Enviroment"},{"location":"env-setup/#demo-environment","text":"The demo requires us to have three Kubernetes clusters and one Virtual Machine. The following tables shows the environment to component matrix. Components GCP AWS CIVO VM Cluster Name cluster-1 cluster-2 mgmt Gloo Mesh Management Gloo Mesh Agent Kubernetes Istio Blue-Green-Canary Service Navigate to the $TUTORIAL_HOME , cd $TUTORIAL_HOME Create the Ansible variables file, make encrypt-vars Note As we will having sensitive keys in the Ansible variables, its highly recommended to create an encrypted variables file. The following table shows the ansible variables used, do update the values of the file as per your environment. Variable Description Default gcp_vpc_name The name of the VPC that will be created. Same VPC will be used with GKE Cluster kameshs-k8s-vpc gcp_vpn_name The name of the VPN that will be created kameshs-site-to-site-vpn gcp_cred_file The SA JSON file to be copied to VM \u201c$HOME/.config/gcloud/credentials\u201d from the Ansible controller machine gcp_project The GCP project to use gcp_region The GCP region to use asia-south-1 aws_access_key_id The AWS Acess Key aws_secret_access_key The AWS Secret Key aws_region The AWS Region to use ap-south-1 eks_cluster_name The AWS EKS cluster name kameshs-gloo-demos aws_vpc_name The AWS VPC name. The same VPC will be used with EKS cluster kameshs-gloo-demos civo_api_key The CIVO Cloud API Key civo_k3s_cluster_name The CIVO Kubernetes cluster Name gloo-mgmt force_app_install Force install application on VM no clean_istio_vm_files Clean the generated Istio VM files no k8s_context The kubernetes context that will be used to query Istio resources","title":"Demo Environment"},{"location":"env-setup/#vm-setup","text":"We will use Vagrant to run and configure our workload VM. The Workload VM also serves as our Ansible target host to run the ansible playbooks. make vm-up The make vm-up will bring up the vagrant VM with with base packages installed.","title":"VM Setup"},{"location":"env-setup/#cloud-setup","text":"The cloud setup will setup the Clouds and create Kubernetes clusters on them.","title":"Cloud Setup"},{"location":"env-setup/#google-cloud","text":"The following command will run setup Google Cloud with VPC, VPN and setup Kubernetes using the VPC. This task will also setup a site-to-site VPN using strongswan which will allow communication between on-premise(VM) to the GKE. make cloud-gcp-run","title":"Google Cloud"},{"location":"env-setup/#civo-cloud","text":"The following command will setup a Kubernetes Cluster on Civo cloud, make cloud-civo-run","title":"Civo Cloud"},{"location":"env-setup/#amzon-cloud","text":"The following command will setup a Kubernetes Cluster(EKS) on AWS cloud, make cloud-civo-run Note The step above will take approx 30-40 mins to complete","title":"Amzon Cloud"},{"location":"env-setup/#deploy-istio","text":"We will deploy istio to all our workload clusters, in our setup AWS and GCP are workload clusters, run the following command to install the istio, cd $TUTORIAL_HOME /work/mgmt Install Istio on cluster-1 : $TUTORIAL_HOME /bin/install_istio.sh \" $CLUSTER1 \" Install Istio on cluster-2 : $TUTORIAL_HOME /bin/install_istio.sh \" $CLUSTER2 \"","title":"Deploy Istio"},{"location":"env-setup/#ensure-environment","text":"Make sure you have the Gloo Mesh Gateway Enterprise License Key before proceeding to install. Export the license key to variable, export GLOO_MESH_GATEWAY_LICENSE_KEY = <your Gloo Mesh Gateway EE License Key>","title":"Ensure Environment"},{"location":"env-setup/#download-meshctl","text":"Download and install latest meshctl by running, curl -sL https://run.solo.io/meshctl/install | sh Add meshctl to the system path, export PATH = $HOME /.gloo-mesh/bin: $PATH Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config Navigate to management work folder, cd $TUTORIAL_HOME /work/mgmt","title":"Download meshctl"},{"location":"env-setup/#install-gloo-mesh-enterprise","text":"Create the gloo-mesh namespace to install Gloo Mesh , $TUTORIAL_HOME /bin/install_mesh.sh Note You can safely ignore the helm warnings It will take few minutes for the Gloo Mesh to be installed and ready Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status \ud83d\udfe1 Gloo Mesh Agents Connectivity Hints: * No registered clusters detected. To register a remote cluster that has a deployed Gloo Mesh agent, add a KubernetesCluster CR. For more info, see: https://docs.solo.io/gloo-mesh/latest/setup/cluster_registration/enterprise_cluster_registration/ Management Configuration --------------------------- 2021-09-14T06:41:10.594255Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources","title":"Install Gloo Mesh Enterprise"},{"location":"env-setup/#cluster-registrations","text":"To connect our hetregenous Isito clusters, we need to register them with the management cluster. The cluster registration will communicate with Management cluster via an enterprise-agent , we need to pass the enterprise-agent address as part of cluster registrations to enable effective communcation. Register $CLUSTER1 as cluster1 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER1 cluster1 Register $CLUSTER2 as cluster2 : $TUTORIAL_HOME /bin/1_register_cluster.sh $CLUSTER2 cluster2","title":"Cluster Registrations"},{"location":"env-setup/#verify-install","text":"Let us verify if all components are installed, meshctl check server Gloo Mesh Management Cluster Installation -------------------------------------------- \ud83d\udfe2 Gloo Mesh Pods Status +----------+------------+-------------------------------+-----------------+ | CLUSTER | REGISTERED | DASHBOARDS AND AGENTS PULLING | AGENTS PUSHING | +----------+------------+-------------------------------+-----------------+ | cluster1 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ | cluster2 | true | 2 | 1 | +----------+------------+-------------------------------+-----------------+ \ud83d\udfe2 Gloo Mesh Agents Connectivity Management Configuration --------------------------- 2021-09-14T04:56:04.288180Z info klog apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition \ud83d\udfe2 Gloo Mesh CRD Versions \ud83d\udfe2 Gloo Mesh Networking Configuration Resources","title":"Verify Install"},{"location":"env-setup/#mesh-dashboard","text":"We will us the Gloo Mesh Dashboard to verify our registered clusters, kubectl --context = $MGMT port-forward -n gloo-mesh deployment/dashboard 8090 :8090 You can then open the dashboard in your browser which will open a page as shown:","title":"Mesh Dashboard"},{"location":"mesh/","text":"At the end of this chapter you would have, Created Virtual Mesh to connect Cluster-1 and Cluster-2 Applied Access Policies Create TrafficPolicy to distribute traffic Integrated VM Workload with Mesh Ensure environment \u00b6 Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config cd $TUTORIAL_HOME /work/mgmt Enable PeerAuthentication \u00b6 Let us configure Istio PeerAuthentication in $CLUSTER1 and $CLUSTER2 . PeerAuthentication enable the mTLS between service mesh services and will help in unifying the ROOT CA between heterogenous service meshes and make the service across each other to be treated as on mesh. TODO better explanation , Cluster 1 \u00b6 kubectl --context = ${ CLUSTER1 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml Cluster 2 \u00b6 kubectl --context = ${ CLUSTER2 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml Virtual Mesh \u00b6 Virtual Mesh allows seamless communication between meshes, kubectl --context = ${ MGMT } apply -f $TUTORIAL_HOME /mesh-files/bgc-virtual-mesh.yaml","title":"Mesh"},{"location":"mesh/#ensure-environment","text":"Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config cd $TUTORIAL_HOME /work/mgmt","title":"Ensure environment"},{"location":"mesh/#enable-peerauthentication","text":"Let us configure Istio PeerAuthentication in $CLUSTER1 and $CLUSTER2 . PeerAuthentication enable the mTLS between service mesh services and will help in unifying the ROOT CA between heterogenous service meshes and make the service across each other to be treated as on mesh. TODO better explanation ,","title":"Enable PeerAuthentication"},{"location":"mesh/#cluster-1","text":"kubectl --context = ${ CLUSTER1 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml","title":"Cluster 1"},{"location":"mesh/#cluster-2","text":"kubectl --context = ${ CLUSTER2 } apply -f $TUTORIAL_HOME /mesh-files/peer-auth.yaml","title":"Cluster 2"},{"location":"mesh/#virtual-mesh","text":"Virtual Mesh allows seamless communication between meshes, kubectl --context = ${ MGMT } apply -f $TUTORIAL_HOME /mesh-files/bgc-virtual-mesh.yaml","title":"Virtual Mesh"},{"location":"site-site-vpn/","text":"Demo Architecture \u00b6","title":"Site to Site VPN"},{"location":"site-site-vpn/#demo-architecture","text":"","title":"Demo Architecture"},{"location":"tools-and-sources/","text":"At the end of this chapter you will have the required tools and enviroment ready for running the demo. Pre-requsites \u00b6 Access to Google Cloud with Service Account that can create/update/delete, VPC VPN Compute Kubernetes Clusters Access to Google Cloud AWS with permissions to create/update/delete, VPC VPN Compute Kubernetes Clusters CIVO Cloud Account with Access Token Download Tools \u00b6 We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows helm brew install helm Install choco install kubernetes-helm yq v4 brew install yq Download Download jq brew install jq Install choco install yq pipx brew install pipx && pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath kubectl brew install kubectl Download choco install kubernetes-cli kustomize brew install kustomize Download choco install kustomize stern brew install stern Download Download vagrant brew install vagrant Download Download Ansible Install Install N.A Important You will need Gloo Mesh Enterprise License Key to run the demo exercises. If you dont have one, get a trial license from solo.io . Demo Sources \u00b6 Clone the demo sources from the GitHub respository, git clone https://github.com/kameshsampath/gloo-bgc-demo cd gloo-bgc-demo For convinience, we will refer the clone demo sources folder as $TUTORIAL_HOME , export TUTORIAL_HOME = \" $PWD \"","title":"Tools and Sources"},{"location":"tools-and-sources/#pre-requsites","text":"Access to Google Cloud with Service Account that can create/update/delete, VPC VPN Compute Kubernetes Clusters Access to Google Cloud AWS with permissions to create/update/delete, VPC VPN Compute Kubernetes Clusters CIVO Cloud Account with Access Token","title":"Pre-requsites"},{"location":"tools-and-sources/#download-tools","text":"We will be using the following tools as part of the tutorial. Please have them installed and configured before proceeding further. Tool macos linux windows helm brew install helm Install choco install kubernetes-helm yq v4 brew install yq Download Download jq brew install jq Install choco install yq pipx brew install pipx && pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath python3 -m pip install --user pipx && python3 -m pipx ensurepath kubectl brew install kubectl Download choco install kubernetes-cli kustomize brew install kustomize Download choco install kustomize stern brew install stern Download Download vagrant brew install vagrant Download Download Ansible Install Install N.A Important You will need Gloo Mesh Enterprise License Key to run the demo exercises. If you dont have one, get a trial license from solo.io .","title":"Download Tools"},{"location":"tools-and-sources/#demo-sources","text":"Clone the demo sources from the GitHub respository, git clone https://github.com/kameshsampath/gloo-bgc-demo cd gloo-bgc-demo For convinience, we will refer the clone demo sources folder as $TUTORIAL_HOME , export TUTORIAL_HOME = \" $PWD \"","title":"Demo Sources"},{"location":"traffic/","text":"At the end of this chapter you would have, Applied Access Policies Create TrafficPolicy to distribute traffic Integrated VM Workload with Mesh Ensure environment \u00b6 Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config cd $TUTORIAL_HOME /work/mgmt Gloo Mesh Gateway \u00b6 We will create Virtual Gateway, Host and RouteTable to route requests to the servies across the mesh cluster. Important If you have installed the test gateway and virtual service, make sure to delete from the clusters before proceeding further $TUTORIAL_HOME /bin/7_deploy_gateway.sh Let us verify if Gateway and Virtual Services are created on both the clusters that are part of the mesh, kubectl --context = ${ CLUSTER1 } get gw,vs -A kubectl --context = ${ CLUSTER2 } get gw,vs -A Both the commands should return an output like, NAMESPACE NAME AGE istio-system gateway.networking.istio.io/bgc-virtualgateway-17072781039916753854 42s istio-system gateway.networking.istio.io/cross-network-gateway 5d12h istio-system gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 16h istio-system gateway.networking.istio.io/istiod-gateway 5d12h NAMESPACE NAME GATEWAYS HOSTS AGE gloo-mesh virtualservice.networking.istio.io/bgc-virtualhost-gloo-mesh [\"istio-system/bgc-virtualgateway-17072781039916753854\"] [\"*\"] 42s istio-system virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 37h Calling Service \u00b6 Cluster -1 \u00b6 Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER1 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \" Cluster- 2 \u00b6 Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER2 = $( kubectl --context ${ CLUSTER2 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER2 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \" Traffic Policy \u00b6 As we have unified the mesh we are good to distribute traffic amongst them. As part of the next section we will apply various traffic policies to distribute traffic amongst the blue , green and canary services. Green \u00b6 As we already have traffic sent to blue , let use try sending all the traffic to green $TUTORIAL_HOME /bin/8_green.sh Now if you try to call the service via browser or cli as described it should return response from green service. Canary \u00b6 Let us now try sending all the traffic to canary service on the VM, $TUTORIAL_HOME /bin/9_canary.sh Now if you try to call the service via browser or cli as described it should return response from canary service. Blue \u2190 \u2192 Green \u00b6 Let\u2019s try to split the traffic between blue ( 50% ) and green ( 50% ), $TUTORIAL_HOME /bin/11_blue-green.sh If you try check your browser you should see an alternating blue-green traffic. Blue,Green and Canary \u00b6 Finally let\u2019s try to split the traffic between blue ( 40% ), green ( 40% ) and canary ( 20% ), $TUTORIAL_HOME /bin/12_blue-green-canary.sh If you try check your browser you should see almost equal traffic to blue and green and few requests to canary .","title":"Traffic Management"},{"location":"traffic/#ensure-environment","text":"Set cluster environment variables export MGMT = $( export KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER1 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-1/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) export CLUSTER2 = $( export KUBECONFIG = $TUTORIAL_HOME /work/cluster-2/kubeconfig && kubectl config get-contexts -o name | tr -d '\\n\\r' ) Tip It will be a nice convinience to have a merged kubeconfig to easier context switch. To merge kube config run the following command, mkdir -p $TUTORIAL_HOME /work/.kube KUBECONFIG = $TUTORIAL_HOME /work/mgmt/kubeconfig: $TUTORIAL_HOME /work/cluster-1/kubeconfig: $TUTORIAL_HOME /work/cluster-2/kubeconfig ; kubectl config view --flatten > $TUTORIAL_HOME /work/.kube/config export KUBECONFIG = $TUTORIAL_HOME /work/.kube/config cd $TUTORIAL_HOME /work/mgmt","title":"Ensure environment"},{"location":"traffic/#gloo-mesh-gateway","text":"We will create Virtual Gateway, Host and RouteTable to route requests to the servies across the mesh cluster. Important If you have installed the test gateway and virtual service, make sure to delete from the clusters before proceeding further $TUTORIAL_HOME /bin/7_deploy_gateway.sh Let us verify if Gateway and Virtual Services are created on both the clusters that are part of the mesh, kubectl --context = ${ CLUSTER1 } get gw,vs -A kubectl --context = ${ CLUSTER2 } get gw,vs -A Both the commands should return an output like, NAMESPACE NAME AGE istio-system gateway.networking.istio.io/bgc-virtualgateway-17072781039916753854 42s istio-system gateway.networking.istio.io/cross-network-gateway 5d12h istio-system gateway.networking.istio.io/istio-ingressgateway-istio-system-cluster1-gloo-mesh 16h istio-system gateway.networking.istio.io/istiod-gateway 5d12h NAMESPACE NAME GATEWAYS HOSTS AGE gloo-mesh virtualservice.networking.istio.io/bgc-virtualhost-gloo-mesh [\"istio-system/bgc-virtualgateway-17072781039916753854\"] [\"*\"] 42s istio-system virtualservice.networking.istio.io/istiod-vs [\"istiod-gateway\"] [\"istiod.istio-system.svc.cluster.local\"] 37h","title":"Gloo Mesh Gateway"},{"location":"traffic/#calling-service","text":"","title":"Calling Service"},{"location":"traffic/#cluster-1","text":"Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER1 = $( kubectl --context ${ CLUSTER1 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER1 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER1 } \"","title":"Cluster -1"},{"location":"traffic/#cluster-2","text":"Retrive the Istio Ingress Gateway url to access the application, SVC_GW_CLUSTER2 = $( kubectl --context ${ CLUSTER2 } -n istio-system get svc istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].*}' ) Open the URL in the browser open http://$SVC_GW_CLUSTER2 . Or Poll the service using the script, $TUTORIAL_HOME /bin/call_bgc_service.sh \" ${ CLUSTER2 } \"","title":"Cluster- 2"},{"location":"traffic/#traffic-policy","text":"As we have unified the mesh we are good to distribute traffic amongst them. As part of the next section we will apply various traffic policies to distribute traffic amongst the blue , green and canary services.","title":"Traffic Policy"},{"location":"traffic/#green","text":"As we already have traffic sent to blue , let use try sending all the traffic to green $TUTORIAL_HOME /bin/8_green.sh Now if you try to call the service via browser or cli as described it should return response from green service.","title":"Green"},{"location":"traffic/#canary","text":"Let us now try sending all the traffic to canary service on the VM, $TUTORIAL_HOME /bin/9_canary.sh Now if you try to call the service via browser or cli as described it should return response from canary service.","title":"Canary"},{"location":"traffic/#blue-green","text":"Let\u2019s try to split the traffic between blue ( 50% ) and green ( 50% ), $TUTORIAL_HOME /bin/11_blue-green.sh If you try check your browser you should see an alternating blue-green traffic.","title":"Blue &lt;-- --&gt; Green"},{"location":"traffic/#bluegreen-and-canary","text":"Finally let\u2019s try to split the traffic between blue ( 40% ), green ( 40% ) and canary ( 20% ), $TUTORIAL_HOME /bin/12_blue-green-canary.sh If you try check your browser you should see almost equal traffic to blue and green and few requests to canary .","title":"Blue,Green and Canary"},{"location":"troubleshooting/","text":"Certifcate Request failed \u00b6 When CertificatRequest fails, e.g. kubectl get certificaterequests.cert-manager.io -n step-certificates-system ${ FRUITS_UI_IP } .nip.io -o json | jq '.status.conditions[]' If the command gives an output like, { \"lastTransitionTime\" : \"2021-09-02T04:19:17Z\" , \"message\" : \"Certificate request has been approved by cert-manager.io\" , \"reason\" : \"cert-manager.io\" , \"status\" : \"True\" , \"type\" : \"Approved\" } { \"lastTransitionTime\" : \"2021-09-02T04:19:17Z\" , \"message\" : \"Failed to sign certificate request: The request lacked necessary authorization to be completed. Please see the certificate authority logs for more info.\" , \"reason\" : \"Failed\" , \"status\" : \"False\" , \"type\" : \"Ready\" } Check the certificate authority logs for messages in our case the CA step-ca and lets check its logs, stern -n step-certificates-system step-certificates-0 -i 'level=warning|error' Tip stern is useful kubernetes log viewer","title":"Troubleshooting"},{"location":"troubleshooting/#certifcate-request-failed","text":"When CertificatRequest fails, e.g. kubectl get certificaterequests.cert-manager.io -n step-certificates-system ${ FRUITS_UI_IP } .nip.io -o json | jq '.status.conditions[]' If the command gives an output like, { \"lastTransitionTime\" : \"2021-09-02T04:19:17Z\" , \"message\" : \"Certificate request has been approved by cert-manager.io\" , \"reason\" : \"cert-manager.io\" , \"status\" : \"True\" , \"type\" : \"Approved\" } { \"lastTransitionTime\" : \"2021-09-02T04:19:17Z\" , \"message\" : \"Failed to sign certificate request: The request lacked necessary authorization to be completed. Please see the certificate authority logs for more info.\" , \"reason\" : \"Failed\" , \"status\" : \"False\" , \"type\" : \"Ready\" } Check the certificate authority logs for messages in our case the CA step-ca and lets check its logs, stern -n step-certificates-system step-certificates-0 -i 'level=warning|error' Tip stern is useful kubernetes log viewer","title":"Certifcate Request failed"}]}